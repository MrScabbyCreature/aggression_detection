{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.sparse import hstack\n",
    "from IPython.display import display\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change boolean values in the below cell and run the entire notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_classifier_and_vectorizer_dumping = False\n",
    "do_predictions_again = False #if false, will pick up from a dumped file\n",
    "do_metric_table_dumping = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NAG': 1, 'CAG': 2, 'OAG': 3} \n",
      " {1: 'NAG', 2: 'CAG', 3: 'OAG'}\n",
      "\n",
      "\n",
      "general data\n",
      " NAG    5052\n",
      "CAG    4240\n",
      "OAG    2708\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "\n",
      "OAG_singular_data\n",
      " OAG    3200\n",
      "NAG    3000\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "\n",
      "NAG_singular_data\n",
      " NAG    5000\n",
      "OAG    4700\n",
      "Name: Label, dtype: int64\n",
      "Didn't do jack!\n",
      "CPU times: user 132 ms, sys: 16 ms, total: 148 ms\n",
      "Wall time: 144 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.5/site-packages/pandas/core/generic.py:4619: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#dictionary format for dumping classifiers and vectorizers\n",
    "dump_dictionary_format = {\n",
    "    'OAG_singular_vectorizer': 0,\n",
    "    'OAG_singular_classifier' : 0,\n",
    "    'NAG_singular_vectorizer': 0,\n",
    "    'NAG_singular_classifier' : 0,\n",
    "    'general_vectorizer': 0,\n",
    "    'general_classifier' : 0,\n",
    "}\n",
    "\n",
    "#aggression label to label_nums\n",
    "dic_aggression_level = {\n",
    "    'NAG' : 1,\n",
    "    'CAG' : 2,\n",
    "    'OAG' : 3\n",
    "}\n",
    "\n",
    "dic_reverse_aggression_level = {}\n",
    "for i in dic_aggression_level:\n",
    "    dic_reverse_aggression_level[dic_aggression_level[i]] = i\n",
    "    \n",
    "print(dic_aggression_level, '\\n', dic_reverse_aggression_level)\n",
    "\n",
    "#declaring the required TfidfVectorizer's\n",
    "max_features = 20000\n",
    "\n",
    "feature_dict = {\n",
    "    'unigram' : TfidfVectorizer(max_features=max_features, min_df=3),\n",
    "    '1-2_gram'  : TfidfVectorizer(ngram_range=(1, 2), max_features=max_features, min_df=3),\n",
    "    '1-3_gram'  : TfidfVectorizer(ngram_range=(1, 3), max_features=max_features, min_df=3),\n",
    "    '1-4_gram'  : TfidfVectorizer(ngram_range=(1, 4), max_features=max_features, min_df=3),\n",
    "    'unigram_without_stopwords' : TfidfVectorizer(stop_words='english', \n",
    "                                                  max_features=max_features, min_df=3),\n",
    "    '1-2_gram_without_stopwords'  : TfidfVectorizer(ngram_range=(1, 2), stop_words='english',\n",
    "                                                  max_features=max_features, min_df=3),\n",
    "    '1-3_gram_without_stopwords'  : TfidfVectorizer(ngram_range=(1, 3), stop_words='english',\n",
    "                                                  max_features=max_features, min_df=3),\n",
    "    '1-4_gram_without_stopwords'  : TfidfVectorizer(ngram_range=(1, 4), stop_words='english',\n",
    "                                                  max_features=max_features, min_df=3),\n",
    "\t'1-4_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,4), \n",
    "                                    max_features=max_features, min_df=3),\n",
    "\t'1-5_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,5), \n",
    "                                    max_features=max_features, min_df=3),\n",
    "\t'1-6_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,6), \n",
    "                                    max_features=max_features, min_df=3),\n",
    "\t'1-7_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,7), \n",
    "                                    max_features=max_features, min_df=3),\n",
    "\t'1-8_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,8), \n",
    "                                    max_features=max_features, min_df=3),\n",
    "\t'2-4_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(2,4), \n",
    "                                    max_features=max_features, min_df=3),\n",
    "\t'2-5_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(2,5), \n",
    "                                    max_features=max_features, min_df=3),\n",
    "\t'2-6_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(2,6), \n",
    "                                    max_features=max_features, min_df=3),\n",
    "\t'2-7_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(2,7), \n",
    "                                    max_features=max_features, min_df=3),\n",
    "\t'2-8_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(2,8), \n",
    "                                    max_features=max_features, min_df=3),\n",
    "\t'3-4_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(3,4), \n",
    "                                    max_features=max_features, min_df=3),\n",
    "\t'3-7_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(3,7), \n",
    "                                    max_features=max_features, min_df=3),\n",
    "\t'4-4_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(4,4), \n",
    "                                    max_features=max_features, min_df=3),\n",
    "\t'4-5_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(4,5), \n",
    "                                    max_features=max_features, min_df=3),\n",
    "}\n",
    "\n",
    "\n",
    "#train data\n",
    "train_pd = pd.read_csv(\"train.csv\")\n",
    "train_pd.drop('ID',1,inplace=True)\n",
    "train_pd = shuffle(train_pd)\n",
    "#data for general classifier\n",
    "general_data = train_pd[['Data', 'Label']]\n",
    "general_data['Label_num'] = general_data.Label.map(dic_aggression_level)\n",
    "print(\"\\n\\ngeneral data\\n\", general_data['Label'].value_counts())\n",
    "#data for OAG singular classifier\n",
    "OAG_singular_data = general_data[['Data', 'Label']]\n",
    "OAG_singular_data.Label.replace('CAG', \"NAG\", inplace=True)\n",
    "OAG_singular_data['Label_num'] = OAG_singular_data.Label.map(dic_aggression_level)\n",
    "OAG_singular_data = OAG_singular_data.append(OAG_singular_data[OAG_singular_data['Label']=='OAG'][:500])\n",
    "OAG_singular_data = OAG_singular_data[OAG_singular_data['Label'] == 'OAG'][:3200].append(\n",
    "                    OAG_singular_data[OAG_singular_data['Label'] == 'NAG'][:3000])\n",
    "print(\"\\n\\nOAG_singular_data\\n\", OAG_singular_data['Label'].value_counts())\n",
    "#data for NAG singular classifier\n",
    "NAG_singular_data = general_data[['Data', 'Label']]\n",
    "NAG_singular_data.Label.replace('CAG', \"OAG\", inplace=True)\n",
    "NAG_singular_data['Label_num'] = NAG_singular_data.Label.map(dic_aggression_level)\n",
    "NAG_singular_data = NAG_singular_data[NAG_singular_data['Label'] == 'NAG'][:5000].append(\n",
    "                    NAG_singular_data[NAG_singular_data['Label'] == 'OAG'][:4700])\n",
    "print(\"\\n\\nNAG_singular_data\\n\", NAG_singular_data['Label'].value_counts())\n",
    "\n",
    "save_folder = 'binary_ensemble/'\n",
    "\n",
    "#dumper function\n",
    "from copy import deepcopy\n",
    "def dump_vectorizer_and_classifier(vectorizer, save_path):\n",
    "    print(\"Started\", save_path)\n",
    "    #general\n",
    "    general_vectorizer = deepcopy(vectorizer)\n",
    "    general_classifier = SVC(kernel='linear')\n",
    "    general_classifier.fit(general_vectorizer.fit_transform(general_data['Data']), \n",
    "                           general_data['Label_num'])\n",
    "    #OAG\n",
    "    OAG_singular_vectorizer = deepcopy(vectorizer)\n",
    "    OAG_singular_classifier = SVC(kernel='linear')\n",
    "    OAG_singular_classifier.fit(OAG_singular_vectorizer.fit_transform(OAG_singular_data['Data']),\n",
    "                                OAG_singular_data['Label_num'])\n",
    "    #NAG\n",
    "    NAG_singular_vectorizer = deepcopy(vectorizer)\n",
    "    NAG_singular_classifier = SVC(kernel='linear')\n",
    "    NAG_singular_classifier.fit(NAG_singular_vectorizer.fit_transform(NAG_singular_data['Data']),\n",
    "                                NAG_singular_data['Label_num'])\n",
    "    #dumping\n",
    "    pickle.dump({\n",
    "                'OAG_singular_vectorizer': OAG_singular_vectorizer,\n",
    "                'OAG_singular_classifier' : OAG_singular_classifier,\n",
    "                'NAG_singular_vectorizer': NAG_singular_vectorizer,\n",
    "                'NAG_singular_classifier' : NAG_singular_classifier,\n",
    "                'general_vectorizer': general_vectorizer,\n",
    "                'general_classifier' : general_classifier,\n",
    "                }, \n",
    "        open(save_path, 'wb'))\n",
    "    print(\"Completed\", save_path)\n",
    "\n",
    "if do_classifier_and_vectorizer_dumping:\n",
    "    process_pool = Pool(processes=30)\n",
    "    for label in feature_dict:\n",
    "        process_pool.apply_async(dump_vectorizer_and_classifier, args=(feature_dict[label],\n",
    "                                                      os.path.join(save_folder, label+'.pickle'),))\n",
    "    process_pool.close()\n",
    "    process_pool.join()\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "general data\n",
      " NAG    1233\n",
      "CAG    1057\n",
      "OAG     711\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "\n",
      "OAG_singular_data\n",
      " NAG    2290\n",
      "OAG     711\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "\n",
      "NAG_singular_data\n",
      " OAG    1768\n",
      "NAG    1233\n",
      "Name: Label, dtype: int64\n",
      "CPU times: user 124 ms, sys: 4 ms, total: 128 ms\n",
      "Wall time: 124 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "read_folder = 'binary_ensemble/'\n",
    "save_folder = 'binary_ensemble/results/'\n",
    "feature_file_paths = list(filter(lambda x: x.split('.')[-1]=='pickle', os.listdir(read_folder)))\n",
    "if \"prediction.pickle\" in feature_file_paths:\n",
    "    feature_file_paths.remove(\"prediction.pickle\")\n",
    "# print(feature_file_paths)\n",
    "#test data\n",
    "test_pd = pd.read_csv(\"valid.csv\")\n",
    "test_pd.drop('ID',1,inplace=True)\n",
    "test_pd = shuffle(test_pd)\n",
    "test_pd['Label_num'] = test_pd.Label.map(dic_aggression_level)\n",
    "y_test = test_pd['Label_num']\n",
    "#test data for general classifier\n",
    "general_data = test_pd[['Data', 'Label', 'Label_num']]\n",
    "print(\"\\n\\ngeneral data\\n\", general_data['Label'].value_counts())\n",
    "#test data for OAG singular classifier\n",
    "OAG_singular_data = general_data[['Data', 'Label']]\n",
    "OAG_singular_data.Label.replace('CAG', \"NAG\", inplace=True)\n",
    "OAG_singular_data['Label_num'] = OAG_singular_data.Label.map(dic_aggression_level)\n",
    "print(\"\\n\\nOAG_singular_data\\n\", OAG_singular_data['Label'].value_counts())\n",
    "#test data for NAG singular classifier\n",
    "NAG_singular_data = general_data[['Data', 'Label']]\n",
    "NAG_singular_data.Label.replace('CAG', \"OAG\", inplace=True)\n",
    "NAG_singular_data['Label_num'] = NAG_singular_data.Label.map(dic_aggression_level)\n",
    "print(\"\\n\\nNAG_singular_data\\n\", NAG_singular_data['Label'].value_counts())\n",
    "\n",
    "def evaluate(OAG_singular_prediction, NAG_singular_prediction, general_prediction, save_path, feature_label_list):\n",
    "    OAG = dic_aggression_level['OAG']\n",
    "    NAG = dic_aggression_level['NAG']\n",
    "    CAG = dic_aggression_level['CAG']\n",
    "    \n",
    "    map_dict = {\n",
    "        (OAG, OAG, OAG): OAG,\n",
    "        (NAG, NAG, OAG): NAG,\n",
    "        (OAG, NAG, OAG): OAG,\n",
    "        (NAG, OAG, OAG): CAG,\n",
    "        \n",
    "        (OAG, OAG, NAG): OAG,\n",
    "        (NAG, NAG, NAG): NAG,\n",
    "        (OAG, NAG, NAG): NAG,\n",
    "        (NAG, OAG, NAG): CAG,\n",
    "        \n",
    "        (OAG, OAG, CAG): OAG,\n",
    "        (NAG, NAG, CAG): NAG,\n",
    "        (OAG, NAG, CAG): CAG,\n",
    "        (NAG, OAG, CAG): CAG,\n",
    "    }\n",
    "    y_pred = np.array(list(map(lambda x: map_dict[x], list(zip(OAG_singular_prediction, NAG_singular_prediction, general_prediction)))))\n",
    "    pickle.dump({\n",
    "        \"OAG_singular_feature\": feature_label_list[0],\n",
    "        \"NAG_singular_feature\": feature_label_list[1],\n",
    "        \"general_feature\": feature_label_list[2],\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "        \"recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "        \"f1_score\": f1_score(y_test, y_pred, average='weighted'),\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_pred)\n",
    "    }, open(save_path, 'wb'))\n",
    "#     print(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "if do_metric_table_dumping:\n",
    "    count = 0\n",
    "    data_dict = {}\n",
    "    if do_predictions_again:\n",
    "        for path in feature_file_paths:\n",
    "            label = path.split('.')[0]\n",
    "            data = pickle.load(open(os.path.join(read_folder, path), 'rb'))\n",
    "            data_dict[label] = {'OAG_singular_predictions' : data['OAG_singular_classifier'].predict(data['OAG_singular_vectorizer'].transform(OAG_singular_data['Data'])),\n",
    "                                'NAG_singular_predictions' : data['NAG_singular_classifier'].predict(data['NAG_singular_vectorizer'].transform(NAG_singular_data['Data'])),\n",
    "                                'general_predictions' : data['general_classifier'].predict(data['general_vectorizer'].transform(general_data['Data']))}\n",
    "            count += 1\n",
    "            print(\"Completed predicting {} out of {}.\".format(count, len(feature_file_paths)))\n",
    "    else:\n",
    "        data_dict = pickle.load(open(\"binary_ensemble/prediction.pickle\", 'rb'))\n",
    "    count = 0\n",
    "    print(\"Finished predicting.\")\n",
    "    for OAG in data_dict:\n",
    "        for NAG in data_dict:\n",
    "            for general in data_dict:\n",
    "                evaluate(data_dict[OAG]['OAG_singular_predictions'], data_dict[NAG]['NAG_singular_predictions'],\n",
    "                    data_dict[general]['general_predictions'], os.path.join(save_folder, str(count)+'.pickle'), \n",
    "                         [OAG, NAG, general])\n",
    "                count += 1\n",
    "    if do_predictions_again:\n",
    "        pickle.dump(data_dict, open(\"binary_ensemble/prediction.pickle\", 'wb'))\n",
    "    del data_dict\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OAG_singular_feature</th>\n",
       "      <th>NAG_singular_feature</th>\n",
       "      <th>general_feature</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>confusion_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6187</th>\n",
       "      <td>3-4_char_gram</td>\n",
       "      <td>1-3_gram_without_stopwords</td>\n",
       "      <td>4-5_char_gram</td>\n",
       "      <td>0.571809</td>\n",
       "      <td>0.576140</td>\n",
       "      <td>0.571809</td>\n",
       "      <td>0.564828</td>\n",
       "      <td>[[855, 210, 168], [306, 391, 360], [111, 130, 470]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>3-4_char_gram</td>\n",
       "      <td>1-3_gram_without_stopwords</td>\n",
       "      <td>4-4_char_gram</td>\n",
       "      <td>0.571476</td>\n",
       "      <td>0.575882</td>\n",
       "      <td>0.571476</td>\n",
       "      <td>0.564238</td>\n",
       "      <td>[[857, 208, 168], [307, 389, 361], [115, 127, 469]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9656</th>\n",
       "      <td>4-4_char_gram</td>\n",
       "      <td>1-3_gram_without_stopwords</td>\n",
       "      <td>4-5_char_gram</td>\n",
       "      <td>0.570810</td>\n",
       "      <td>0.574602</td>\n",
       "      <td>0.570810</td>\n",
       "      <td>0.563911</td>\n",
       "      <td>[[855, 212, 166], [307, 391, 359], [111, 133, 467]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5868</th>\n",
       "      <td>4-4_char_gram</td>\n",
       "      <td>1-3_gram_without_stopwords</td>\n",
       "      <td>4-4_char_gram</td>\n",
       "      <td>0.570810</td>\n",
       "      <td>0.574658</td>\n",
       "      <td>0.570810</td>\n",
       "      <td>0.563873</td>\n",
       "      <td>[[856, 212, 165], [306, 391, 360], [114, 131, 466]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8799</th>\n",
       "      <td>3-4_char_gram</td>\n",
       "      <td>1-4_gram_without_stopwords</td>\n",
       "      <td>4-5_char_gram</td>\n",
       "      <td>0.570810</td>\n",
       "      <td>0.574729</td>\n",
       "      <td>0.570810</td>\n",
       "      <td>0.563691</td>\n",
       "      <td>[[855, 211, 167], [308, 389, 360], [111, 131, 469]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9606</th>\n",
       "      <td>3-4_char_gram</td>\n",
       "      <td>1-3_gram_without_stopwords</td>\n",
       "      <td>2-7_char_gram</td>\n",
       "      <td>0.570477</td>\n",
       "      <td>0.574602</td>\n",
       "      <td>0.570477</td>\n",
       "      <td>0.563681</td>\n",
       "      <td>[[850, 214, 169], [307, 391, 359], [107, 133, 471]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>3-4_char_gram</td>\n",
       "      <td>1-3_gram_without_stopwords</td>\n",
       "      <td>1-6_char_gram</td>\n",
       "      <td>0.570477</td>\n",
       "      <td>0.574201</td>\n",
       "      <td>0.570477</td>\n",
       "      <td>0.563638</td>\n",
       "      <td>[[849, 216, 168], [309, 391, 357], [105, 134, 472]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10388</th>\n",
       "      <td>3-4_char_gram</td>\n",
       "      <td>1-3_gram_without_stopwords</td>\n",
       "      <td>1-5_char_gram</td>\n",
       "      <td>0.570477</td>\n",
       "      <td>0.574502</td>\n",
       "      <td>0.570477</td>\n",
       "      <td>0.563575</td>\n",
       "      <td>[[850, 213, 170], [309, 390, 358], [105, 134, 472]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8240</th>\n",
       "      <td>3-4_char_gram</td>\n",
       "      <td>unigram_without_stopwords</td>\n",
       "      <td>4-5_char_gram</td>\n",
       "      <td>0.570810</td>\n",
       "      <td>0.574033</td>\n",
       "      <td>0.570810</td>\n",
       "      <td>0.563555</td>\n",
       "      <td>[[858, 210, 165], [315, 393, 349], [123, 126, 462]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8833</th>\n",
       "      <td>4-4_char_gram</td>\n",
       "      <td>1-3_gram_without_stopwords</td>\n",
       "      <td>1-5_char_gram</td>\n",
       "      <td>0.570143</td>\n",
       "      <td>0.573594</td>\n",
       "      <td>0.570143</td>\n",
       "      <td>0.563552</td>\n",
       "      <td>[[850, 217, 166], [308, 393, 356], [106, 137, 468]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      OAG_singular_feature        NAG_singular_feature general_feature  \\\n",
       "6187         3-4_char_gram  1-3_gram_without_stopwords   4-5_char_gram   \n",
       "3195         3-4_char_gram  1-3_gram_without_stopwords   4-4_char_gram   \n",
       "9656         4-4_char_gram  1-3_gram_without_stopwords   4-5_char_gram   \n",
       "5868         4-4_char_gram  1-3_gram_without_stopwords   4-4_char_gram   \n",
       "8799         3-4_char_gram  1-4_gram_without_stopwords   4-5_char_gram   \n",
       "9606         3-4_char_gram  1-3_gram_without_stopwords   2-7_char_gram   \n",
       "2196         3-4_char_gram  1-3_gram_without_stopwords   1-6_char_gram   \n",
       "10388        3-4_char_gram  1-3_gram_without_stopwords   1-5_char_gram   \n",
       "8240         3-4_char_gram   unigram_without_stopwords   4-5_char_gram   \n",
       "8833         4-4_char_gram  1-3_gram_without_stopwords   1-5_char_gram   \n",
       "\n",
       "       accuracy  precision    recall  f1_score  \\\n",
       "6187   0.571809   0.576140  0.571809  0.564828   \n",
       "3195   0.571476   0.575882  0.571476  0.564238   \n",
       "9656   0.570810   0.574602  0.570810  0.563911   \n",
       "5868   0.570810   0.574658  0.570810  0.563873   \n",
       "8799   0.570810   0.574729  0.570810  0.563691   \n",
       "9606   0.570477   0.574602  0.570477  0.563681   \n",
       "2196   0.570477   0.574201  0.570477  0.563638   \n",
       "10388  0.570477   0.574502  0.570477  0.563575   \n",
       "8240   0.570810   0.574033  0.570810  0.563555   \n",
       "8833   0.570143   0.573594  0.570143  0.563552   \n",
       "\n",
       "                                          confusion_matrix  \n",
       "6187   [[855, 210, 168], [306, 391, 360], [111, 130, 470]]  \n",
       "3195   [[857, 208, 168], [307, 389, 361], [115, 127, 469]]  \n",
       "9656   [[855, 212, 166], [307, 391, 359], [111, 133, 467]]  \n",
       "5868   [[856, 212, 165], [306, 391, 360], [114, 131, 466]]  \n",
       "8799   [[855, 211, 167], [308, 389, 360], [111, 131, 469]]  \n",
       "9606   [[850, 214, 169], [307, 391, 359], [107, 133, 471]]  \n",
       "2196   [[849, 216, 168], [309, 391, 357], [105, 134, 472]]  \n",
       "10388  [[850, 213, 170], [309, 390, 358], [105, 134, 472]]  \n",
       "8240   [[858, 210, 165], [315, 393, 349], [123, 126, 462]]  \n",
       "8833   [[850, 217, 166], [308, 393, 356], [106, 137, 468]]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric_table = {\n",
    "        \"OAG_singular_feature\": [],\n",
    "        \"NAG_singular_feature\": [],\n",
    "        \"general_feature\": [],\n",
    "        \"accuracy\": [],\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1_score\": [],\n",
    "        \"confusion_matrix\": []\n",
    "}\n",
    "\n",
    "for file in os.listdir(\"binary_ensemble/results/\"):\n",
    "    data_dict = pickle.load(open(os.path.join(\"binary_ensemble/results/\", file), 'rb'))\n",
    "    for key in data_dict:\n",
    "        metric_table[key].append(data_dict[key])\n",
    "        \n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "metric_table = pd.DataFrame.from_dict(metric_table)\n",
    "metric_table = metric_table[['OAG_singular_feature', 'NAG_singular_feature', 'general_feature', 'accuracy', 'precision', 'recall', 'f1_score', 'confusion_matrix']]\n",
    "display(metric_table.sort_values(\"f1_score\", ascending=False).head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
