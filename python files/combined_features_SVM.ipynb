{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data locations(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to features: ['combined_features/part_features_3.pickle', 'combined_features/part_features_2.pickle', 'combined_features/part_features_7.pickle', 'combined_features/part_features_10.pickle', 'combined_features/part_features_4.pickle', 'combined_features/part_features_9.pickle', 'combined_features/part_features_0.pickle', 'combined_features/part_features_8.pickle', 'combined_features/part_features_6.pickle', 'combined_features/part_features_5.pickle', 'combined_features/part_features_1.pickle']\n",
      "\n",
      "Write Folder: combined_features/folder_0\n"
     ]
    }
   ],
   "source": [
    "#folder that contains the features and to-be-dumped files\n",
    "folder = 'combined_features'\n",
    "#path to the features dumped file\n",
    "path_to_features_list = list(map(lambda z: os.path.join(folder, z), list(filter(lambda x: x.split('.')[-1] == 'pickle', os.listdir(folder)))))\n",
    "#create a new folder for dumping new files\n",
    "count = 0\n",
    "folder_to_be_made = True\n",
    "while os.path.exists(os.path.join(folder, 'folder_'+str(count))):\n",
    "    if len(os.listdir(os.path.join(folder, 'folder_'+str(count)))) == 0:\n",
    "        folder_to_be_made = False\n",
    "        break\n",
    "    count += 1\n",
    "write_folder = os.path.join(folder, 'folder_'+str(count))\n",
    "if folder_to_be_made:\n",
    "    os.makedirs(write_folder)\n",
    "print(\"Path to features: {}\\n\\nWrite Folder: {}\".format(path_to_features_list, write_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN models and save accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickler_function(save_path, feature_list, data):\n",
    "    model = SVC(kernel='linear')\n",
    "#     model = GridSearchCV(estimator=model_classifier, param_grid={'kernel': ['rbf', 'linear']}, n_jobs=1)\n",
    "#     model = RandomForestClassifier()\n",
    "#     print(model)\n",
    "#     return\n",
    "    print(\"Trying to fit\", feature_list)\n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "    %time model.fit(X_train, y_train)\n",
    "    print(\"Finished fitting\", feature_list)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Finished predicting\", feature_list)\n",
    "    evaluation_dict = {\n",
    "        'Feature 1': feature_list[0], \n",
    "        'Feature 2': feature_list[1], \n",
    "        'Feature 3': feature_list[2], \n",
    "        'Feature 4': feature_list[3], \n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "        'F1 score': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'Confusion Matrix': confusion_matrix(y_test, y_pred)\n",
    "    }\n",
    "    print(\"Finished evaluating\", feature_list)\n",
    "    pickle.dump(evaluation_dict, open(save_path, 'wb'))\n",
    "    print(\"Dumping successful at\", save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump the classifier evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Data', 'Feature 3', 'Feature 4', 'Feature 1', 'Feature 2'])\n",
      "48\n",
      "Trying to fit ['1-5_char_gram', 'unigram_without_stopwords', 'capitals_char_1_gram', '-']\n",
      "Trying to fit ['1-5_char_gram', 'char_4_gram_non_alphabet', '1-4_gram', '-']\n",
      "Trying to fit ['1-5_char_gram', 'char_1_gram_non_alphabet', '1-4_gram', '-']\n",
      "Trying to fit ['1-5_char_gram', 'char_5_gram_non_alphabet', '1-4_gram', '-']\n",
      "Trying to fit ['char_2_gram_non_alphabet', '1-5_char_gram', '1-4_gram', '-']\n",
      "Trying to fit ['1-5_char_gram', 'capitals_char_2_gram', '1-4_gram', '-']\n",
      "Trying to fit ['1-5_char_gram', 'capitals_char_3_gram', '1-4_gram', '-']\n",
      "Trying to fit ['capitals_char_1_gram', '1-5_char_gram', '1-4_gram', '-']\n",
      "Trying to fit ['1-5_char_gram', 'capitals_char_2_gram', 'char_4_gram_non_alphabet', '-']\n",
      "Trying to fit ['1-5_char_gram', 'char_4_gram_non_alphabet', 'capitals_char_3_gram', '-']\n",
      "Trying to fit ['1-5_char_gram', 'char_4_gram_non_alphabet', 'capitals_char_1_gram', '-']\n",
      "Trying to fit ['1-5_char_gram', 'char_1_gram_non_alphabet', 'capitals_char_2_gram', '-']\n",
      "Trying to fit ['1-5_char_gram', 'char_1_gram_non_alphabet', 'capitals_char_3_gram', '-']\n",
      "Trying to fit ['1-5_char_gram', 'char_1_gram_non_alphabet', 'capitals_char_1_gram', '-']\n",
      "Trying to fit ['1-5_char_gram', 'char_5_gram_non_alphabet', 'capitals_char_2_gram', '-']\n",
      "Trying to fit ['1-5_char_gram', 'char_5_gram_non_alphabet', 'capitals_char_3_gram', '-']\n",
      "Trying to fit ['1-5_char_gram', 'char_5_gram_non_alphabet', 'capitals_char_1_gram', '-']\n",
      "Trying to fit ['char_2_gram_non_alphabet', '1-5_char_gram', 'capitals_char_2_gram', '-']\n",
      "Trying to fit ['char_2_gram_non_alphabet', '1-5_char_gram', 'capitals_char_3_gram', '-']\n",
      "Trying to fit ['char_2_gram_non_alphabet', '1-5_char_gram', 'capitals_char_1_gram', '-']\n",
      "Trying to fit ['2-6_char_gram', '1-2_gram', 'char_4_gram_non_alphabet', '-']\n",
      "Trying to fit ['2-6_char_gram', '1-2_gram', 'char_1_gram_non_alphabet', '-']\n",
      "Trying to fit ['2-6_char_gram', '1-2_gram', 'char_5_gram_non_alphabet', '-']\n",
      "Trying to fit ['2-6_char_gram', '1-2_gram', 'char_2_gram_non_alphabet', '-']\n",
      "Trying to fit ['2-6_char_gram', '1-2_gram', 'capitals_char_2_gram', '-']\n",
      "Trying to fit ['2-6_char_gram', '1-2_gram', 'capitals_char_3_gram', '-']\n",
      "Trying to fit ['2-6_char_gram', '1-2_gram', 'capitals_char_1_gram', '-']\n",
      "Trying to fit ['2-6_char_gram', 'unigram', 'char_4_gram_non_alphabet', '-']\n",
      "Trying to fit ['2-6_char_gram', 'unigram', 'char_1_gram_non_alphabet', '-']\n",
      "Trying to fit ['2-6_char_gram', 'unigram', 'char_5_gram_non_alphabet', '-']\n",
      "Trying to fit ['2-6_char_gram', 'unigram', 'char_2_gram_non_alphabet', '-']\n",
      "Trying to fit ['2-6_char_gram', 'unigram', 'capitals_char_2_gram', '-']\n",
      "Trying to fit ['2-6_char_gram', 'unigram', 'capitals_char_3_gram', '-']\n",
      "Trying to fit ['2-6_char_gram', 'unigram', 'capitals_char_1_gram', '-']\n",
      "Trying to fit ['2-6_char_gram', 'unigram_without_stopwords', 'char_4_gram_non_alphabet', '-']\n",
      "Trying to fit ['2-6_char_gram', 'char_1_gram_non_alphabet', 'unigram_without_stopwords', '-']\n",
      "Trying to fit ['2-6_char_gram', 'char_5_gram_non_alphabet', 'unigram_without_stopwords', '-']\n",
      "Trying to fit ['2-6_char_gram', 'char_2_gram_non_alphabet', 'unigram_without_stopwords', '-']\n",
      "Trying to fit ['2-6_char_gram', 'capitals_char_2_gram', 'unigram_without_stopwords', '-']\n",
      "Trying to fit ['2-6_char_gram', 'unigram_without_stopwords', 'capitals_char_3_gram', '-']\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for path in path_to_features_list:\n",
    "    data_dict = pickle.load(open(path, 'rb'))\n",
    "\n",
    "    print(data_dict.keys())\n",
    "    print(len(data_dict['Data']))\n",
    "\n",
    "    process_pool = Pool(processes=40)\n",
    "\n",
    "    for i in range(len(data_dict['Data'])):\n",
    "        process_pool.apply_async(pickler_function, args=(os.path.join(write_folder, \"SVM_\"+str(count)+'.pickle'), [data_dict['Feature '+str(j)][i] for j in range(1, 5)], data_dict['Data'][i]))\n",
    "        count += 1\n",
    "        \n",
    "    process_pool.close()\n",
    "    process_pool.join()\n",
    "\n",
    "print(\"\\n\\n\\nDONE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
