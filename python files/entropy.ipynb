{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from IPython.display import display\n",
    "from nltk import TweetTokenizer\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label conversion dictionaries: text to num, num to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CAG': 2, 'OAG': 3, 'NAG': 1} \n",
      " {1: 'NAG', 2: 'CAG', 3: 'OAG'}\n"
     ]
    }
   ],
   "source": [
    "dic_aggression_level = {\n",
    "    'NAG' : 1,\n",
    "    'CAG' : 2,\n",
    "    'OAG' : 3\n",
    "}\n",
    "\n",
    "dic_reverse_aggression_level = {}\n",
    "for i in dic_aggression_level:\n",
    "    dic_reverse_aggression_level[dic_aggression_level[i]] = i\n",
    "    \n",
    "print(dic_aggression_level, '\\n', dic_reverse_aggression_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARING DATA WITH PANDAS\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATA\n",
      "NAG    6285\n",
      "CAG    5297\n",
      "OAG    3419\n",
      "Name: Label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Label</th>\n",
       "      <th>Label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>Focus on making cash available  then only  peo...</td>\n",
       "      <td>CAG</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5103</th>\n",
       "      <td>She's so ignorant Megha Mukherji</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5090</th>\n",
       "      <td>Sonia I am holding Rel cap 430, please suggest...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9074</th>\n",
       "      <td>why dont u make ur room sound proof..simple</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6769</th>\n",
       "      <td>Showing everything and saying bold...</td>\n",
       "      <td>CAG</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Then what happens in pantry coach dedicated fo...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>We should respect every religion. May be he wa...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7272</th>\n",
       "      <td>Car is good.. bt i must say.. i only heard 'aa...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10305</th>\n",
       "      <td>friends we have to understand the ground reali...</td>\n",
       "      <td>CAG</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Bad...........</td>\n",
       "      <td>CAG</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Data Label  Label_num\n",
       "2930   Focus on making cash available  then only  peo...   CAG          2\n",
       "5103                    She's so ignorant Megha Mukherji   OAG          3\n",
       "5090   Sonia I am holding Rel cap 430, please suggest...   NAG          1\n",
       "9074         why dont u make ur room sound proof..simple   OAG          3\n",
       "6769               Showing everything and saying bold...   CAG          2\n",
       "140    Then what happens in pantry coach dedicated fo...   OAG          3\n",
       "1756   We should respect every religion. May be he wa...   OAG          3\n",
       "7272   Car is good.. bt i must say.. i only heard 'aa...   NAG          1\n",
       "10305  friends we have to understand the ground reali...   CAG          2\n",
       "45                                        Bad...........   CAG          2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "TEST DATA\n",
      "NAG    630\n",
      "OAG    144\n",
      "CAG    142\n",
      "Name: Label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Label</th>\n",
       "      <th>Label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>What will be done to the money we have ?</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Unchange the rapo rate could lead the stagnate...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PK Movie Bhagawan Shiv Ko Aapman Kiya..   I Ha...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>Thousands of people have died due to bandhs an...</td>\n",
       "      <td>CAG</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>Worst F.M ever.what about 5  lakhs tax limit? ...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>Pak army rape modi daughter and go back. Now m...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>I am clean without cash as transactions are th...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>I have 5000 shares of Pnb @75.90 please tell m...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>these bhagwa terrorists can't digest their mea...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>Worst Decision and very worst implementation. ...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Data Label  Label_num\n",
       "156           What will be done to the money we have ?   NAG          1\n",
       "211  Unchange the rapo rate could lead the stagnate...   NAG          1\n",
       "13   PK Movie Bhagawan Shiv Ko Aapman Kiya..   I Ha...   NAG          1\n",
       "798  Thousands of people have died due to bandhs an...   CAG          2\n",
       "640  Worst F.M ever.what about 5  lakhs tax limit? ...   NAG          1\n",
       "568  Pak army rape modi daughter and go back. Now m...   NAG          1\n",
       "321  I am clean without cash as transactions are th...   NAG          1\n",
       "119  I have 5000 shares of Pnb @75.90 please tell m...   NAG          1\n",
       "820  these bhagwa terrorists can't digest their mea...   OAG          3\n",
       "721  Worst Decision and very worst implementation. ...   OAG          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train data\n",
    "train_pd = shuffle(pd.concat((pd.read_csv(\"train.csv\")[['Data', 'Label']], pd.read_csv(\"valid.csv\")[['Data', 'Label']])), random_state=20)\n",
    "# train_pd['Label'].replace('CAG', 'OAG', inplace=True)\n",
    "train_pd['Label_num'] = train_pd.Label.map(dic_aggression_level)\n",
    "\n",
    "#test data\n",
    "# test_fb_pd = shuffle(pd.read_csv(\"test_fb.csv\")[['Data', 'Label']], random_state=20)\n",
    "# test_fb_pd['Label_num'] = test_fb_pd.Label.map(dic_aggression_level)\n",
    "# test_tw_pd = shuffle(pd.read_csv(\"test_tw.csv\")[['Data', 'Label']], random_state=20)\n",
    "# test_tw_pd['Label_num'] = test_tw_pd.Label.map(dic_aggression_level)\n",
    "\n",
    "#test data\n",
    "test_pd = pd.read_csv(\"test_fb.csv\")\n",
    "test_pd.drop('ID',1,inplace=True)\n",
    "test_pd = shuffle(test_pd, random_state = 20)\n",
    "\n",
    "# merge binary classification (CAG -> OAG)\n",
    "# test_pd['Label'].replace('CAG', 'OAG', inplace=True)\n",
    "\n",
    "test_pd['Label_num'] = test_pd.Label.map(dic_aggression_level)\n",
    "\n",
    "\n",
    "print(\"TRAIN DATA\")\n",
    "print(train_pd.Label.value_counts())\n",
    "display(train_pd.head(10))\n",
    "\n",
    "print(\"\\n\\n\\nTEST DATA\")\n",
    "print(test_pd.Label.value_counts())\n",
    "display(test_pd.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "\n",
    "feature_dict = {\n",
    "    'unigram' : TfidfVectorizer(max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    'bigram'  : TfidfVectorizer(ngram_range=(2,2), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    'trigram'  : TfidfVectorizer(ngram_range=(3,3), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    'quadgram'  : TfidfVectorizer(ngram_range=(4,4), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    'fivegram'  : TfidfVectorizer(ngram_range=(5,5), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '1-2_gram'  : TfidfVectorizer(ngram_range=(1, 2), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '1-3_gram'  : TfidfVectorizer(ngram_range=(1, 3), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '1-4_gram'  : TfidfVectorizer(ngram_range=(1, 4), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '2-3_gram'  : TfidfVectorizer(ngram_range=(2, 3), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '2-4_gram'  : TfidfVectorizer(ngram_range=(2, 4), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '3-4_gram'  : TfidfVectorizer(ngram_range=(3, 4), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    'unigram_without_stopwords' : TfidfVectorizer(stop_words='english', \n",
    "                                                  max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    'bigram_without_stopwords'  : TfidfVectorizer(ngram_range=(2,2), stop_words='english',\n",
    "                                                  max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    'trigram_without_stopwords'  : TfidfVectorizer(ngram_range=(3,3), stop_words='english',\n",
    "                                                  max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    'quadgram_without_stopwords'  : TfidfVectorizer(ngram_range=(4,4), stop_words='english',\n",
    "                                                  max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    'fivegram_without_stopwords'  : TfidfVectorizer(ngram_range=(5,5), stop_words='english',\n",
    "                                                  max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '1-2_gram_without_stopwords'  : TfidfVectorizer(ngram_range=(1, 2), stop_words='english',\n",
    "                                                  max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '1-3_gram_without_stopwords'  : TfidfVectorizer(ngram_range=(1, 3), stop_words='english',\n",
    "                                                  max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '1-4_gram_without_stopwords'  : TfidfVectorizer(ngram_range=(1, 4), stop_words='english',\n",
    "                                                  max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '2-3_gram_without_stopwords'  : TfidfVectorizer(ngram_range=(2, 3), stop_words='english',\n",
    "                                                  max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '2-4_gram_without_stopwords'  : TfidfVectorizer(ngram_range=(2, 4), stop_words='english',\n",
    "                                                  max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '3-4_gram_without_stopwords'  : TfidfVectorizer(ngram_range=(3, 4), stop_words='english',\n",
    "                                                  max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '1-1_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,1), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'1-2_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,2), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'1-3_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,3), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'1-4_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,4), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'1-5_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,5), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'1-6_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,6), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'1-7_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,7), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'1-8_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,8), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'2-2_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(2,2), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'2-3_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(2,3), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'2-4_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(2,4), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'2-5_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(2,5), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'2-6_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(2,6), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'2-7_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(2,7), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'2-8_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(2,8), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'3-3_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(3,3), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'3-4_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(3,4), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'3-5_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(3,5), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'3-6_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(3,6), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'3-7_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(3,7), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'3-8_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(3,8), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'4-4_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(4,4), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'4-5_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(4,5), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'4-6_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(4,6), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'4-7_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(4,7), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'4-8_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(4,8), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'5-5_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(5,5), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'5-6_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(5,6), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'5-7_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(5,7), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'5-8_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(5,8), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'6-6_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(6,6), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'6-7_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(6,7), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'6-8_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(6,8), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'7-7_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(7,7), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'7-8_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(7,8), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'8-8_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(8,8), \n",
    "                                    max_features=max_features, min_df=3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0986122886681096 0.0 1.4426950408889634 0.1143182336230831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.5/site-packages/ipykernel_launcher.py:50: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/opt/anaconda/lib/python3.5/site-packages/ipykernel_launcher.py:51: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from nltk import TweetTokenizer\n",
    "\n",
    "word_count_dict = {}\n",
    "word_set = set()\n",
    "tweeter = TweetTokenizer()\n",
    "\n",
    "#NAG\n",
    "for row_number in range(len(train_pd)):\n",
    "    row = train_pd.iloc[row_number]\n",
    "    for word in set(tweeter.tokenize(row.Data)):\n",
    "        if word not in word_set:\n",
    "            word_set.add(word)\n",
    "            word_count_dict[word] = [0, 0, 0]\n",
    "        word_count_dict[word][row.Label_num-1] += 1\n",
    "\n",
    "from functools import reduce\n",
    "def entropy(x):\n",
    "    if x == 0:\n",
    "        return -0\n",
    "    return x*np.log(x)\n",
    "\n",
    "word_entropy_dict = {}\n",
    "_good = []\n",
    "_bad = []\n",
    "_all = []\n",
    "\n",
    "for word in word_count_dict:\n",
    "    total_words = np.sum(word_count_dict[word])\n",
    "    word_entropy_dict[word] = np.abs(np.sum(list(map(lambda x: entropy(x/total_words), np.array(word_count_dict[word])))))\n",
    "    \n",
    "# conversion_ratio = len(word_entropy_dict.items())/np.sum(list(zip(*word_entropy_dict.items()))[1])\n",
    "entropy_list = list(zip(*word_entropy_dict.items()))[1]\n",
    "max_entropy = max(entropy_list)\n",
    "min_entropy = min(entropy_list)\n",
    "count_list = np.log(np.sum(np.array(list(zip(*word_count_dict.items()))[1]), axis=1))\n",
    "count_list = count_list[count_list != 0]\n",
    "count_list = 1/count_list\n",
    "max_count = max(count_list)\n",
    "min_count = min(count_list)\n",
    "print(max_entropy, min_entropy, max_count, min_count)\n",
    "del entropy_list\n",
    "del count_list\n",
    "def map_count_to_entropy_range(x):\n",
    "    return min_entropy + (x - min_count) * (max_entropy - min_entropy) / (max_count - min_count)\n",
    "    \n",
    "count_weight = 7\n",
    "for word in word_count_dict:\n",
    "    _all.append((word, word_count_dict[word], (word_entropy_dict[word]) + count_weight*map_count_to_entropy_range(1/np.log(np.sum(word_count_dict[word])))))\n",
    "    word_entropy_dict[word] += count_weight*map_count_to_entropy_range(1/np.log(np.sum(word_count_dict[word])))\n",
    "#     if word_entropy_dict[word] + 1/np.log(np.sum(word_count_dict[word])) < 1.0986122886681096 and total_words > 3:\n",
    "#         _good.append((word, word_count_dict[word], word_entropy_dict[word]+ conversion_ratio/np.log(np.sum(word_count_dict[word]))))\n",
    "#     else:\n",
    "#         _bad.append((word, word_count_dict[word], word_entropy_dict[word]+ conversion_ratio/np.log(np.sum(word_count_dict[word]))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20928</th>\n",
       "      <td>sheep</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19937</th>\n",
       "      <td>emerged</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>Pvt</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7178</th>\n",
       "      <td>peopl</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>92</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18592</th>\n",
       "      <td>RAPE</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28137</th>\n",
       "      <td>insects</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>Mahatma</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>rejecting</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3235</th>\n",
       "      <td>FEW</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20862</th>\n",
       "      <td>memorable</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23230</th>\n",
       "      <td>unbelievable</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23063</th>\n",
       "      <td>eternally</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>borrowing</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21967</th>\n",
       "      <td>#shameonjournalism</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3244</th>\n",
       "      <td>comming</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29469</th>\n",
       "      <td>identified</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19137</th>\n",
       "      <td>Monitoring</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>Greatest</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>apartment</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20929</th>\n",
       "      <td>HARMONY</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26184</th>\n",
       "      <td>prosecution</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6340</th>\n",
       "      <td>appointments</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2953</th>\n",
       "      <td>yard</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839</th>\n",
       "      <td>civilization</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11683</th>\n",
       "      <td>Hereafter</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21377</th>\n",
       "      <td>samjh</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26079</th>\n",
       "      <td>isreal</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3010</th>\n",
       "      <td>widely</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28006</th>\n",
       "      <td>sacking</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6498</th>\n",
       "      <td>ghar</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19122</th>\n",
       "      <td>ilk</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25831</th>\n",
       "      <td>honoured</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17242</th>\n",
       "      <td>possibilities</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28092</th>\n",
       "      <td>butt</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6497</th>\n",
       "      <td>Shabana</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29001</th>\n",
       "      <td>Busy</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3165</th>\n",
       "      <td>costlier</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23282</th>\n",
       "      <td>Image</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13755</th>\n",
       "      <td>swift</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23247</th>\n",
       "      <td>lagta</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25826</th>\n",
       "      <td>projected</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6973</th>\n",
       "      <td>untold</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17087</th>\n",
       "      <td>yourselves</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6958</th>\n",
       "      <td>endorses</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15485</th>\n",
       "      <td>grammar</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11364</th>\n",
       "      <td>mujhe</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13674</th>\n",
       "      <td>maharashtra</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21378</th>\n",
       "      <td>Loss</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6899</th>\n",
       "      <td>sanctioned</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3139</th>\n",
       "      <td>CORRUPTION</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19157</th>\n",
       "      <td>DD</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>FINANCE</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20935</th>\n",
       "      <td>answerable</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>financing</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29003</th>\n",
       "      <td>extensively</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>abundantly</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6908</th>\n",
       "      <td>Dan</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26049</th>\n",
       "      <td>CIRCLE</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3019</th>\n",
       "      <td>peculiar</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3018</th>\n",
       "      <td>wll</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29002</th>\n",
       "      <td>counts</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23142</th>\n",
       "      <td>bluffing</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11371</th>\n",
       "      <td>Stand</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23266</th>\n",
       "      <td>Hold</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6927</th>\n",
       "      <td>refuge</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11621</th>\n",
       "      <td>LET'S</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16979</th>\n",
       "      <td>53</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23145</th>\n",
       "      <td>guy's</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23146</th>\n",
       "      <td>ANYTHING</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23297</th>\n",
       "      <td>cadres</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21868</th>\n",
       "      <td>COULD</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15496</th>\n",
       "      <td>pets</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971</th>\n",
       "      <td>layer</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17282</th>\n",
       "      <td>retained</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21865</th>\n",
       "      <td>SLEEP</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7054</th>\n",
       "      <td>SIGNAL</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23148</th>\n",
       "      <td>Press</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21870</th>\n",
       "      <td>Fax</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3191</th>\n",
       "      <td>2013-14</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28737</th>\n",
       "      <td>ruckus</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6859</th>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19980</th>\n",
       "      <td>finalised</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17291</th>\n",
       "      <td>hadn't</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17052</th>\n",
       "      <td>thou</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19850</th>\n",
       "      <td>repercussions</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29686</th>\n",
       "      <td>#CNBCTV18</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26036</th>\n",
       "      <td>cleanse</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26121</th>\n",
       "      <td>notification</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6572</th>\n",
       "      <td>focused</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6452</th>\n",
       "      <td>LORD</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15493</th>\n",
       "      <td>re-elected</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26102</th>\n",
       "      <td>#surgicalstrike</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>riddance</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18651</th>\n",
       "      <td>polluted</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>Discuss</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>vailed</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6460</th>\n",
       "      <td>Logistics</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18570</th>\n",
       "      <td>Mothers</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3028</th>\n",
       "      <td>😳</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>5.545177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     word      count   entropy\n",
       "20928               sheep  [1, 1, 0]  5.545177\n",
       "19937             emerged  [1, 1, 0]  5.545177\n",
       "848                   Pvt  [1, 1, 0]  5.545177\n",
       "7178                peopl  [1, 1, 0]  5.545177\n",
       "881                    92  [1, 1, 0]  5.545177\n",
       "18592                RAPE  [1, 1, 0]  5.545177\n",
       "28137             insects  [1, 1, 0]  5.545177\n",
       "1340              Mahatma  [1, 1, 0]  5.545177\n",
       "1339            rejecting  [1, 1, 0]  5.545177\n",
       "3235                  FEW  [1, 1, 0]  5.545177\n",
       "20862           memorable  [1, 1, 0]  5.545177\n",
       "23230        unbelievable  [1, 1, 0]  5.545177\n",
       "23063           eternally  [1, 1, 0]  5.545177\n",
       "6818            borrowing  [1, 1, 0]  5.545177\n",
       "21967  #shameonjournalism  [1, 1, 0]  5.545177\n",
       "3244              comming  [1, 1, 0]  5.545177\n",
       "29469          identified  [1, 1, 0]  5.545177\n",
       "19137          Monitoring  [1, 1, 0]  5.545177\n",
       "883              Greatest  [1, 1, 0]  5.545177\n",
       "2919            apartment  [1, 1, 0]  5.545177\n",
       "20929             HARMONY  [1, 1, 0]  5.545177\n",
       "26184         prosecution  [1, 1, 0]  5.545177\n",
       "6340         appointments  [1, 1, 0]  5.545177\n",
       "2953                 yard  [1, 1, 0]  5.545177\n",
       "1839         civilization  [1, 1, 0]  5.545177\n",
       "11683           Hereafter  [1, 1, 0]  5.545177\n",
       "21377               samjh  [1, 1, 0]  5.545177\n",
       "26079              isreal  [1, 1, 0]  5.545177\n",
       "3010               widely  [1, 1, 0]  5.545177\n",
       "28006             sacking  [1, 1, 0]  5.545177\n",
       "6498                 ghar  [1, 1, 0]  5.545177\n",
       "19122                 ilk  [1, 1, 0]  5.545177\n",
       "25831            honoured  [1, 1, 0]  5.545177\n",
       "17242       possibilities  [1, 1, 0]  5.545177\n",
       "28092                butt  [1, 1, 0]  5.545177\n",
       "6497              Shabana  [1, 1, 0]  5.545177\n",
       "29001                Busy  [1, 1, 0]  5.545177\n",
       "3165             costlier  [1, 1, 0]  5.545177\n",
       "23282               Image  [1, 1, 0]  5.545177\n",
       "13755               swift  [1, 1, 0]  5.545177\n",
       "23247               lagta  [1, 1, 0]  5.545177\n",
       "25826           projected  [1, 1, 0]  5.545177\n",
       "6973               untold  [1, 1, 0]  5.545177\n",
       "17087          yourselves  [1, 1, 0]  5.545177\n",
       "6958             endorses  [1, 1, 0]  5.545177\n",
       "15485             grammar  [1, 1, 0]  5.545177\n",
       "11364               mujhe  [1, 1, 0]  5.545177\n",
       "13674         maharashtra  [1, 1, 0]  5.545177\n",
       "21378                Loss  [1, 1, 0]  5.545177\n",
       "6899           sanctioned  [1, 1, 0]  5.545177\n",
       "3139           CORRUPTION  [1, 1, 0]  5.545177\n",
       "19157                  DD  [1, 1, 0]  5.545177\n",
       "21888             FINANCE  [1, 1, 0]  5.545177\n",
       "20935          answerable  [1, 1, 0]  5.545177\n",
       "863             financing  [1, 1, 0]  5.545177\n",
       "29003         extensively  [1, 1, 0]  5.545177\n",
       "3014           abundantly  [1, 1, 0]  5.545177\n",
       "6908                  Dan  [1, 1, 0]  5.545177\n",
       "26049              CIRCLE  [1, 1, 0]  5.545177\n",
       "3019             peculiar  [1, 1, 0]  5.545177\n",
       "3018                  wll  [1, 1, 0]  5.545177\n",
       "29002              counts  [1, 1, 0]  5.545177\n",
       "23142            bluffing  [1, 1, 0]  5.545177\n",
       "11371               Stand  [1, 1, 0]  5.545177\n",
       "23266                Hold  [1, 1, 0]  5.545177\n",
       "6927               refuge  [1, 1, 0]  5.545177\n",
       "11621               LET'S  [1, 1, 0]  5.545177\n",
       "16979                  53  [1, 1, 0]  5.545177\n",
       "23145               guy's  [1, 1, 0]  5.545177\n",
       "23146            ANYTHING  [1, 1, 0]  5.545177\n",
       "23297              cadres  [1, 1, 0]  5.545177\n",
       "21868               COULD  [1, 1, 0]  5.545177\n",
       "15496                pets  [1, 1, 0]  5.545177\n",
       "2971                layer  [1, 1, 0]  5.545177\n",
       "17282            retained  [1, 1, 0]  5.545177\n",
       "21865               SLEEP  [1, 1, 0]  5.545177\n",
       "7054               SIGNAL  [1, 1, 0]  5.545177\n",
       "23148               Press  [1, 1, 0]  5.545177\n",
       "21870                 Fax  [1, 1, 0]  5.545177\n",
       "3191              2013-14  [1, 1, 0]  5.545177\n",
       "28737              ruckus  [1, 1, 0]  5.545177\n",
       "6859              SCIENCE  [1, 1, 0]  5.545177\n",
       "19980           finalised  [1, 1, 0]  5.545177\n",
       "17291              hadn't  [1, 1, 0]  5.545177\n",
       "17052                thou  [1, 1, 0]  5.545177\n",
       "19850       repercussions  [1, 1, 0]  5.545177\n",
       "29686           #CNBCTV18  [1, 1, 0]  5.545177\n",
       "26036             cleanse  [1, 1, 0]  5.545177\n",
       "26121        notification  [1, 1, 0]  5.545177\n",
       "6572              focused  [1, 1, 0]  5.545177\n",
       "6452                 LORD  [1, 1, 0]  5.545177\n",
       "15493          re-elected  [1, 1, 0]  5.545177\n",
       "26102     #surgicalstrike  [1, 1, 0]  5.545177\n",
       "2993             riddance  [1, 1, 0]  5.545177\n",
       "18651            polluted  [1, 1, 0]  5.545177\n",
       "869               Discuss  [1, 1, 0]  5.545177\n",
       "6998               vailed  [1, 1, 0]  5.545177\n",
       "6460            Logistics  [1, 1, 0]  5.545177\n",
       "18570             Mothers  [1, 1, 0]  5.545177\n",
       "3028                    😳  [1, 1, 0]  5.545177"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(_all, columns=['word', 'count', 'entropy']).sort_values('entropy').head(12500).tail(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\r\n",
      "http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/\n",
      "\n",
      "\n",
      "1 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/ \r\n",
      "\r\n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "2 \n",
      " Dear Indian Express, I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal.  A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa).  We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "3 \n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "4 \n",
      " Both hindus and muslims need to ignore and isolate people of both religion or any other religion who create hatred on religious grounds\n",
      "\n",
      "\n",
      "5 \n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "6 \n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\r\n",
      "\r\n",
      "http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/\n",
      "\n",
      "\n",
      "7 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/ \r\n",
      "\r\n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "8 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/\r\n",
      "\r\n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "9 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/\r\n",
      "\r\n",
      "Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake\n",
      "\n",
      "\n",
      "10 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/\r\n",
      "\r\n",
      "Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/\r\n",
      "\r\n",
      "Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake\n",
      "\n",
      "\n",
      "12 \n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "13 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/ \r\n",
      "\r\n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "14 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/\r\n",
      "\r\n",
      "Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake\n",
      "\n",
      "\n",
      "15 \n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "16 \n",
      " On what grounds would the RBI circulate Rs5000 and 10000 notes? What's the math behind the multiplier effect? Sounds very vague when you say that!!!\n",
      "\n",
      "\n",
      "17 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/\r\n",
      "\r\n",
      "Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake\n",
      "\n",
      "\n",
      "18 \n",
      " http://indianexpress.com/.../fatwa-issued-against.../\r\n",
      "\r\n",
      "Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake\n",
      "\n",
      "\n",
      "19 \n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "20 \n",
      " Amongst 10 countries we are the champions and amongst 207 countries we have to search India at the bottom of the database amongst countries which doesn't even have my districts population. Through we say other sports are not getting promoted where sports here mean a battle between willow and leather, which was forcibly created by corporate forces to grow their appetite like how media edits the news here ,what to and what not to debate on.  We here of so many unknown names who have atleast competing with other nations. They come from remotest of Indian villages and not so near from the corners of a MCD's and PVR's. This is more embracing moment for the urban population than rural because ,parents leave them to their passion but urban student gets forcibly converted into an engineer and to an IT professional traveling in H1B. we say infrastructure and bla bla available in our cities but nothing truly came from the so called metros. This elucidates the very fact of the conservative parenting from the urban divide.  Another stakeholder here is the local municipalities,state government and the central government by enlarge to create feeding grounds for the budding athletes,to identify them to groom them to coach them  and to make them compete.  Capital infusion will not be a problem with independent body of governing cricket here alone has funds to buy Boeing. we parents should take oath to take our potential childrens to travel new contours which they have never been.\n",
      "\n",
      "\n",
      "21 \n",
      " https://www.facebook.com/indianexpress/?fref=ts\r\n",
      "\r\n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "22 \n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\r\n",
      "http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/\n",
      "\n",
      "\n",
      "23 \n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "24 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/\r\n",
      "\r\n",
      "Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake\n",
      "\n",
      "\n",
      "25 \n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "26 \n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake. http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 \n",
      " Cow ban - no forced religiousness \r\n",
      "An old man beaten to deat on doubtful grounds by cow vigilants - no forced religiousness.\r\n",
      "Day and might concerts with loud speakers : no forced religiousness. Use of loud speakers on navratre/ganesh chathurthi/holi/ jagran : no forced religiousness.\r\n",
      "1 min azan : forced religiousness \r\n",
      "We muslims can give up cow meat for our hindu friends!! No big deal, but such horrendous statements by hindu celebrities are disgrace to secular status***\n",
      "\n",
      "\n",
      "28 \n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\r\n",
      "\r\n",
      "http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/\n",
      "\n",
      "\n",
      "29 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/\r\n",
      "\r\n",
      "Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake\n",
      "\n",
      "\n",
      "30 \n",
      " Poverty is across the religions and castes. Unfortunately for vote bank politics, such thing is being done. Reservations should be based on economical grounds. Women reservations are also accepted. But not on religion basis. It should be immediately withdrawn. JAI HIND\n",
      "\n",
      "\n",
      "31 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/\r\n",
      "\r\n",
      "Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake\n",
      "\n",
      "\n",
      "32 \n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "33 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/ \r\n",
      "\r\n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "34 \n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "35 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/\r\n",
      "\r\n",
      "Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake\n",
      "\n",
      "\n",
      "36 \n",
      " http://indianexpress.com/.../fatwa-issued-against.../\r\n",
      "\r\n",
      "Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake\n",
      "\n",
      "\n",
      "37 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/\r\n",
      "\r\n",
      "Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake\n",
      "\n",
      "\n",
      "38 \n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\r\n",
      "\r\n",
      "http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/\n",
      "\n",
      "\n",
      "39 \n",
      " Dear Indian Express, I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal.  A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa).  We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "40 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/\r\n",
      "\r\n",
      "Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake\n",
      "\n",
      "\n",
      "41 \n",
      " Dear Indian Express, I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "42 \n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\r\n",
      "\r\n",
      "http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/\n",
      "\n",
      "\n",
      "43 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/ \r\n",
      "\r\n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "44 \n",
      " Be a human first..stop blaming  others on religion grounds stay in sound proof house\n",
      "\n",
      "\n",
      "45 \n",
      " Dear Indian Express, I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal.  A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa).  We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "46 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/ \r\n",
      "\r\n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 \n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "48 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/ \r\n",
      "\r\n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake. Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake\n",
      "\n",
      "\n",
      "49 \n",
      " http://indianexpress.com/article/india/fatwa-issued-against-reality-singing-star-nahid-afrin-by-42-clerics-4569825/ \r\n",
      "\r\n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n",
      "50 \n",
      " Dear Indian Express,\r\n",
      "I feel sorry to write that your journalists are out there to misinterpret events and present them in a distorted manner just to make some sensational headlines. The news about upcoming singer Nahid Afreen has been prepared without proper research and investigation. It was very sad to see that your journalists don't even know the difference between fatwa and appeal. \r\n",
      "A fatwa is a non-binding legal opinion issued only by a qualified jurist (Mufti) on a request by someone who is unable to understand religious guidelines whereas the leaflet that is being circulated and presented as Fatwa is just a hard copy of an appeal signed by 46 men who believe in religious practices. The merit of appeal can be discussed on many grounds and in many dimensions but first and foremost, an appeal can not be called a legal opinion (Fatwa). \r\n",
      "We expect apology from your side for such a mistake.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_search = 'grounds'\n",
    "count = 0\n",
    "\n",
    "for sentence in train_pd['Data']:\n",
    "    if word_search in tweeter.tokenize(sentence):\n",
    "        print(count, '\\n', sentence, end='\\n\\n\\n')\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'hate'\n",
    "print(word_count_dict[word], word_entropy_dict[word] + 1/np.log(np.sum(word_count_dict[word])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06 0\n",
      "0.21 0\n",
      "0.16 0\n",
      "0.01 0\n",
      "0.41 0\n",
      "0.81 1\n",
      "0.36 0\n",
      "0.26 0\n",
      "0.86 2\n",
      "0.11 0\n",
      "0.31 0\n",
      "1.01 7\n",
      "0.96 4\n",
      "1.11 16\n",
      "1.36 145\n",
      "1.61 542\n",
      "0.46 0\n",
      "1.06 8\n",
      "0.71 1\n",
      "1.16 29\n",
      "1.46 282\n",
      "2.26 2264\n",
      "1.66 637\n",
      "2.31 2392\n",
      "1.96 1342\n",
      "2.41 2732\n",
      "0.76 1\n",
      "0.91 4\n",
      "1.51 345\n",
      "2.11 1784\n",
      "2.86 4027\n",
      "1.81 973\n",
      "2.51 3017\n",
      "1.56 442\n",
      "1.31 102\n",
      "2.16 1960\n",
      "0.51 0\n",
      "0.56 0\n",
      "2.71 3620\n",
      "2.81 3900\n",
      "2.46 2851\n",
      "2.91 4163\n",
      "1.26 68\n",
      "2.56 3143\n",
      "1.41 204\n",
      "1.21 48\n",
      "3.16 4867\n",
      "3.06 4731\n",
      "3.01 4487\n",
      "2.61 3319\n",
      "2.36 2594\n",
      "1.76 867\n",
      "1.86 1086\n",
      "1.71 748\n",
      "2.01 1491\n",
      "2.66 3465\n",
      "2.06 1654\n",
      "2.21 2096\n",
      "1.91 1210\n",
      "2.76 3785\n",
      "2.96 4380\n",
      "3.26 5122\n",
      "3.11 4867\n",
      "0.61 0\n",
      "0.66 1\n",
      "3.36 5368\n",
      "3.31 5321\n",
      "3.21 5122\n",
      "3.41 5448\n",
      "3.46 5744\n",
      "\n",
      "2.71 Shape:  (15001, 3620) (15001,) (916, 3620) (916,)\n",
      "\n",
      "1.61 Shape:  (15001, 542) (15001,) (916, 542) (916,)\n",
      "\n",
      "1.96 Shape:  (15001, 1342) (15001,) (916, 1342) (916,)\n",
      "\n",
      "\n",
      "2.31 Shape:  (15001, 2392) (15001,) (916, 2392) (916,)\n",
      "2.86 Shape:  (15001, 4027) (15001,) (916, 4027) (916,)\n",
      "\n",
      "2.16 Shape:  (15001, 1960) (15001,) (916, 1960) (916,)\n",
      "\n",
      "\n",
      "2.76 Shape:  (15001, 3785) (15001,) (916, 3785) (916,)\n",
      "2.36 Shape:  (15001, 2594) (15001,) (916, 2594) (916,)\n",
      "\n",
      "2.21 Shape:  (15001, 2096) (15001,) (916, 2096) (916,)\n",
      "\n",
      "\n",
      "1.51 Shape:  (15001, 345) (15001,) (916, 345) (916,)\n",
      "\n",
      "2.01 Shape:  (15001, 1491) (15001,) (916, 1491) (916,)\n",
      "1.86 Shape:  (15001, 1086) (15001,) (916, 1086) (916,)\n",
      "\n",
      "\n",
      "\n",
      "2.61 Shape:  (15001, 3319) (15001,) (916, 3319) (916,)\n",
      "1.66 Shape:  (15001, 637) (15001,) (916, 637) (916,)\n",
      "2.06 Shape:  (15001, 1654) (15001,) (916, 1654) (916,)\n",
      "\n",
      "3.01 Shape:  (15001, 4487) (15001,) (916, 4487) (916,)\n",
      "\n",
      "\n",
      "1.91 Shape:  (15001, 1210) (15001,) (916, 1210) (916,)\n",
      "1.31 Shape:  (15001, 102) (15001,) (916, 102) (916,)\n",
      "\n",
      "\n",
      "1.56 Shape:  (15001, 442) (15001,) (916, 442) (916,)\n",
      "\n",
      "1.06 Shape:  (15001, 8) (15001,) (916, 8) (916,)\n",
      "1.76 Shape:  (15001, 867) (15001,) (916, 867) (916,)\n",
      "\n",
      "1.81 Shape:  (15001, 973) (15001,) (916, 973) (916,)\n",
      "\n",
      "2.66 Shape:  (15001, 3465) (15001,) (916, 3465) (916,)\n",
      "\n",
      "1.16 Shape:  (15001, 29) (15001,) (916, 29) (916,)\n",
      "\n",
      "1.26 Shape:  (15001, 68) (15001,) (916, 68) (916,)\n",
      "\n",
      "\n",
      "1.41 Shape:  (15001, 204) (15001,) (916, 204) (916,)\n",
      "\n",
      "1.11 Shape:  (15001, 16) (15001,) (916, 16) (916,)\n",
      "\n",
      "\n",
      "2.41 Shape:  (15001, 2732) (15001,) (916, 2732) (916,)\n",
      "1.01 Shape:  (15001, 7) (15001,) (916, 7) (916,)\n",
      "2.91 Shape:  (15001, 4163) (15001,) (916, 4163) (916,)\n",
      "\n",
      "\n",
      "2.11 Shape:  (15001, 1784) (15001,) (916, 1784) (916,)\n",
      "1.21 Shape:  (15001, 48) (15001,) (916, 48) (916,)\n",
      "\n",
      "1.71 Shape:  (15001, 748) (15001,) (916, 748) (916,)\n",
      "\n",
      "\n",
      "2.46 Shape:  (15001, 2851) (15001,) (916, 2851) (916,)\n",
      "\n",
      "2.56 Shape:  (15001, 3143) (15001,) (916, 3143) (916,)\n",
      "\n",
      "1.36 Shape:  (15001, 145) (15001,) (916, 145) (916,)\n",
      "\n",
      "1.46 Shape:  (15001, 282) (15001,) (916, 282) (916,)\n",
      "2.51 Shape:  (15001, 3017) (15001,) (916, 3017) (916,)\n",
      "\n",
      "3.16 Shape:  (15001, 4867) (15001,) (916, 4867) (916,)\n",
      "\n",
      "\n",
      "2.96 Shape:  (15001, 4380) (15001,) (916, 4380) (916,)\n",
      "\n",
      "\n",
      "\n",
      "3.26 Shape:  (15001, 5122) (15001,) (916, 5122) (916,)\n",
      "3.46 Shape:  (15001, 5744) (15001,) (916, 5744) (916,)\n",
      "3.41 Shape:  (15001, 5448) (15001,) (916, 5448) (916,)\n",
      "3.11 Shape:  (15001, 4867) (15001,) (916, 4867) (916,)\n",
      "\n",
      "3.36 Shape:  (15001, 5368) (15001,) (916, 5368) (916,)\n",
      "\n",
      "3.21 Shape:  (15001, 5122) (15001,) (916, 5122) (916,)\n",
      "\n",
      "3.31 Shape:  (15001, 5321) (15001,) (916, 5321) (916,)\n",
      "\n",
      "2.81 Shape:  (15001, 3900) (15001,) (916, 3900) (916,)\n",
      "\n",
      "3.06 Shape:  (15001, 4731) (15001,) (916, 4731) (916,)\n",
      "\n",
      "2.26 Shape:  (15001, 2264) (15001,) (916, 2264) (916,)\n",
      "CPU times: user 3.22 s, sys: 1.13 s, total: 4.35 s\n",
      "Wall time: 5.74 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Abhor/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.06 0.6877729257641921 0.5605393832230803\n",
      "1.06 [[630   0   0]\n",
      " [142   0   0]\n",
      " [144   0   0]]\n",
      "\n",
      "\n",
      "\n",
      "3.51 5744\n",
      "CPU times: user 3.23 s, sys: 1.25 s, total: 4.48 s\n",
      "Wall time: 7.82 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Abhor/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.01 0.6877729257641921 0.5605393832230803\n",
      "1.01 [[630   0   0]\n",
      " [142   0   0]\n",
      " [144   0   0]]\n",
      "\n",
      "\n",
      "\n",
      "3.56 5960\n",
      "\n",
      "3.51 Shape:  (15001, 5744) (15001,) (916, 5744) (916,)\n",
      "\n",
      "3.56 Shape:  (15001, 5960) (15001,) (916, 5960) (916,)\n",
      "CPU times: user 8.45 s, sys: 1.42 s, total: 9.87 s\n",
      "Wall time: 15.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Abhor/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11 0.537117903930131 0.5231388628485344\n",
      "1.11 [[437 193   0]\n",
      " [ 87  55   0]\n",
      " [ 85  59   0]]\n",
      "\n",
      "\n",
      "\n",
      "3.61 6400\n",
      "\n",
      "3.61 Shape:  (15001, 6400) (15001,) (916, 6400) (916,)\n",
      "CPU times: user 15.5 s, sys: 2.36 s, total: 17.9 s\n",
      "Wall time: 26.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Abhor/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16 0.517467248908297 0.5109484534486924\n",
      "1.16 [[410 220   0]\n",
      " [ 78  64   0]\n",
      " [ 81  63   0]]\n",
      "\n",
      "\n",
      "\n",
      "3.66 6400\n",
      "CPU times: user 20.8 s, sys: 1.94 s, total: 22.8 s\n",
      "Wall time: 30 s\n",
      "\n",
      "3.66 Shape:  (15001, 6400) (15001,) (916, 6400) (916,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Abhor/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21 0.47707423580786024 0.4824395505228182\n",
      "1.21 [[368 262   0]\n",
      " [ 73  69   0]\n",
      " [ 71  73   0]]\n",
      "\n",
      "\n",
      "\n",
      "3.71 6442\n",
      "\n",
      "3.71 Shape:  (15001, 6442) (15001,) (916, 6442) (916,)\n",
      "CPU times: user 29.7 s, sys: 1.46 s, total: 31.2 s\n",
      "Wall time: 44.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Abhor/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26 0.527292576419214 0.517277323900262\n",
      "1.26 [[416 214   0]\n",
      " [ 75  67   0]\n",
      " [ 86  58   0]]\n",
      "\n",
      "\n",
      "\n",
      "3.76 6442\n",
      "\n",
      "3.76 Shape:  (15001, 6442) (15001,) (916, 6442) (916,)\n",
      "CPU times: user 1min 14s, sys: 1.5 s, total: 1min 16s\n",
      "Wall time: 1min 51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Abhor/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.31 0.49890829694323147 0.5001572756996882\n",
      "1.31 [[377 253   0]\n",
      " [ 62  80   0]\n",
      " [ 71  73   0]]\n",
      "\n",
      "\n",
      "\n",
      "3.81 6442\n",
      "\n",
      "3.81 Shape:  (15001, 6442) (15001,) (916, 6442) (916,)\n",
      "CPU times: user 1min 45s, sys: 1.68 s, total: 1min 46s\n",
      "Wall time: 2min 32s\n",
      "1.36 0.509825327510917 0.515767931750866\n",
      "1.36 [[381 246   3]\n",
      " [ 58  83   1]\n",
      " [ 64  77   3]]\n",
      "\n",
      "\n",
      "\n",
      "3.86 6442\n",
      "\n",
      "3.86 Shape:  (15001, 6442) (15001,) (916, 6442) (916,)\n",
      "CPU times: user 2min 20s, sys: 1.95 s, total: 2min 22s\n",
      "Wall time: 3min\n",
      "1.41 0.5131004366812227 0.5218682512011191\n",
      "1.41 [[381 244   5]\n",
      " [ 57  84   1]\n",
      " [ 61  78   5]]\n",
      "\n",
      "\n",
      "\n",
      "3.91 6596\n",
      "\n",
      "3.91 Shape:  (15001, 6596) (15001,) (916, 6596) (916,)\n",
      "CPU times: user 3min 8s, sys: 1.71 s, total: 3min 10s\n",
      "Wall time: 3min 59s\n",
      "CPU times: user 3min 15s, sys: 2.44 s, total: 3min 18s\n",
      "Wall time: 4min 4s\n",
      "1.56 0.5393013100436681 0.5740776554857956\n",
      "1.56 [[368 239  23]\n",
      " [ 49  90   3]\n",
      " [ 35  73  36]]\n",
      "\n",
      "\n",
      "\n",
      "3.96 6596\n",
      "\n",
      "3.96 Shape:  (15001, 6596) (15001,) (916, 6596) (916,)\n",
      "1.71 0.5436681222707423 0.5787609741077961\n",
      "1.71 [[368 226  36]\n",
      " [ 51  86   5]\n",
      " [ 35  65  44]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 3min 14s, sys: 1.83 s, total: 3min 16s\n",
      "Wall time: 4min 10s\n",
      "4.01 6825\n",
      "\n",
      "4.01 Shape:  (15001, 6825) (15001,) (916, 6825) (916,)\n",
      "CPU times: user 2min 53s, sys: 1.69 s, total: 2min 55s\n",
      "Wall time: 4min 15s\n",
      "1.66 0.5403930131004366 0.5768855912722028\n",
      "1.66 [[367 232  31]\n",
      " [ 51  86   5]\n",
      " [ 33  69  42]]\n",
      "\n",
      "\n",
      "\n",
      "4.06 6825\n",
      "CPU times: user 3min 17s, sys: 2.14 s, total: 3min 19s\n",
      "Wall time: 4min 20s\n",
      "\n",
      "4.06 Shape:  (15001, 6825) (15001,) (916, 6825) (916,)\n",
      "1.46 0.5502183406113537 0.5726884196068739\n",
      "1.46 [[391 229  10]\n",
      " [ 54  88   0]\n",
      " [ 53  66  25]]\n",
      "\n",
      "\n",
      "\n",
      "4.11 7256\n",
      "\n",
      "4.11 Shape:  (15001, 7256) (15001,) (916, 7256) (916,)\n",
      "1.76 0.5447598253275109 0.5797184999799618\n",
      "1.76 [[358 231  41]\n",
      " [ 44  92   6]\n",
      " [ 36  59  49]]\n",
      "\n",
      "\n",
      "\n",
      "4.16 7256\n",
      "CPU times: user 3min 5s, sys: 1.82 s, total: 3min 7s\n",
      "Wall time: 4min 30s\n",
      "\n",
      "4.16 Shape:  (15001, 7256) (15001,) (916, 7256) (916,)\n",
      "CPU times: user 3min 26s, sys: 1.98 s, total: 3min 28s\n",
      "Wall time: 4min 33s\n",
      "CPU times: user 3min 26s, sys: 3.23 s, total: 3min 30s\n",
      "Wall time: 4min 35s\n",
      "1.51 0.5403930131004366 0.5731988061009591\n",
      "1.51 [[376 234  20]\n",
      " [ 51  86   5]\n",
      " [ 39  72  33]]\n",
      "\n",
      "\n",
      "\n",
      "4.21 7521\n",
      "CPU times: user 3min 34s, sys: 3.56 s, total: 3min 37s\n",
      "Wall time: 4min 37s\n",
      "1.81 0.5469432314410481 0.5811037753567195\n",
      "1.81 [[358 228  44]\n",
      " [ 40  94   8]\n",
      " [ 37  58  49]]\n",
      "\n",
      "\n",
      "\n",
      "4.26 7521\n",
      "CPU times: user 3min 36s, sys: 2.28 s, total: 3min 39s\n",
      "Wall time: 4min 40s\n",
      "\n",
      "4.21 Shape:  (15001, 7521) (15001,) (916, 7521) (916,)\n",
      "1.91 0.5491266375545851 0.5857897938509803\n",
      "1.91 [[356 231  43]\n",
      " [ 42  92   8]\n",
      " [ 32  57  55]]\n",
      "\n",
      "\n",
      "\n",
      "4.31 7521\n",
      "\n",
      "4.26 Shape:  (15001, 7521) (15001,) (916, 7521) (916,)\n",
      "2.06 0.5600436681222707 0.5970263076467153\n",
      "2.06 [[359 218  53]\n",
      " [ 40  89  13]\n",
      " [ 27  52  65]]\n",
      "\n",
      "\n",
      "\n",
      "4.36 7521\n",
      "CPU times: user 3min 11s, sys: 1.81 s, total: 3min 13s\n",
      "Wall time: 4min 46s\n",
      "\n",
      "4.31 Shape:  (15001, 7521) (15001,) (916, 7521) (916,)\n",
      "2.11 0.5600436681222707 0.5961054845965841\n",
      "2.11 [[359 217  54]\n",
      " [ 40  91  11]\n",
      " [ 28  53  63]]\n",
      "\n",
      "\n",
      "\n",
      "4.41 7521\n",
      "CPU times: user 3min 43s, sys: 3.04 s, total: 3min 46s\n",
      "Wall time: 4min 48s\n",
      "\n",
      "4.36 Shape:  (15001, 7521) (15001,) (916, 7521) (916,)\n",
      "\n",
      "4.41 Shape:  (15001, 7521) (15001,) (916, 7521) (916,)\n",
      "CPU times: user 3min 43s, sys: 2.48 s, total: 3min 45s\n",
      "Wall time: 4min 54s\n",
      "1.61 0.5403930131004366 0.577163250990173\n",
      "1.61 [[370 234  26]\n",
      " [ 55  84   3]\n",
      " [ 33  70  41]]\n",
      "\n",
      "\n",
      "\n",
      "4.46 7521\n",
      "CPU times: user 3min 53s, sys: 1.84 s, total: 3min 55s\n",
      "2.21 0.5676855895196506 0.602492568063589\n",
      "Wall time: 4min 54s\n",
      "2.21 [[371 202  57]\n",
      " [ 43  84  15]\n",
      " [ 30  49  65]]\n",
      "\n",
      "\n",
      "\n",
      "4.51 7521\n",
      "CPU times: user 3min 54s, sys: 2.76 s, total: 3min 56s\n",
      "Wall time: 4min 59s\n",
      "\n",
      "4.46 Shape:  (15001, 7521) (15001,) (916, 7521) (916,)\n",
      "\n",
      "4.51 Shape:  (15001, 7521) (15001,) (916, 7521) (916,)\n",
      "CPU times: user 3min 50s, sys: 2.88 s, total: 3min 53s\n",
      "Wall time: 4min 59s\n",
      "CPU times: user 3min 53s, sys: 1.74 s, total: 3min 55s\n",
      "Wall time: 4min 59s\n",
      "2.31 0.5676855895196506 0.6024684465593892\n",
      "2.31 [[377 196  57]\n",
      " [ 45  81  16]\n",
      " [ 30  52  62]]\n",
      "\n",
      "\n",
      "\n",
      "4.56 7936\n",
      "2.66 0.5600436681222707 0.596852568640681\n",
      "2.66 [[367 213  50]\n",
      " [ 44  81  17]\n",
      " [ 31  48  65]]\n",
      "\n",
      "\n",
      "\n",
      "4.61 8467\n",
      "\n",
      "4.56 Shape:  (15001, 7936) (15001,) (916, 7936) (916,)\n",
      "\n",
      "4.61 Shape:  (15001, 8467) (15001,) (916, 8467) (916,)\n",
      "2.41 0.5644104803493449 0.6002437652135604\n",
      "2.41 [[372 204  54]\n",
      " [ 46  83  13]\n",
      " [ 28  54  62]]\n",
      "\n",
      "\n",
      "\n",
      "4.66 8467\n",
      "CPU times: user 3min 34s, sys: 2.1 s, total: 3min 36s\n",
      "Wall time: 5min 6s\n",
      "2.91 0.5644104803493449 0.6007331602837017\n",
      "2.91 [[377 201  52]\n",
      " [ 46  79  17]\n",
      " [ 29  54  61]]\n",
      "2.71 0.5578602620087336 0.5942982788153186\n",
      "\n",
      "\n",
      "\n",
      "2.71 [[365 211  54]\n",
      " [ 46  81  15]\n",
      " [ 30  49  65]]\n",
      "\n",
      "\n",
      "\n",
      "4.71 8467\n",
      "4.76 8467\n",
      "CPU times: user 3min 57s, sys: 2.18 s, total: 3min 59s\n",
      "Wall time: 5min 7s\n",
      "CPU times: user 3min 58s, sys: 2.33 s, total: 4min\n",
      "Wall time: 5min 6s\n",
      "\n",
      "4.66 Shape:  (15001, 8467) (15001,) (916, 8467) (916,)\n",
      "\n",
      "4.76 Shape:  (15001, 8467) (15001,) (916, 8467) (916,)\n",
      "\n",
      "4.71 Shape:  (15001, 8467) (15001,) (916, 8467) (916,)\n",
      "CPU times: user 4min 4s, sys: 3.31 s, total: 4min 7s\n",
      "Wall time: 5min 10s\n",
      "CPU times: user 3min 57s, sys: 3.7 s, total: 4min\n",
      "Wall time: 5min 11s\n",
      "1.86 0.5469432314410481 0.5824435220510413\n",
      "1.86 [[353 231  46]\n",
      " [ 40  96   6]\n",
      " [ 32  60  52]]\n",
      "\n",
      "\n",
      "\n",
      "4.81 8467\n",
      "2.76 0.5567685589519651 0.5937042143102522\n",
      "2.76 [[365 210  55]\n",
      " [ 44  80  18]\n",
      " [ 30  49  65]]\n",
      "\n",
      "\n",
      "\n",
      "2.96 0.5655021834061136 0.6010959774452448\n",
      "CPU times: user 3min 34s, sys: 1.9 s, total: 3min 36s\n",
      "4.86 8467\n",
      "2.96 [[373 202  55]\n",
      " [ 46  81  15]\n",
      " [ 29  51  64]]\n",
      "Wall time: 5min 16s\n",
      "\n",
      "\n",
      "\n",
      "4.91 8467\n",
      "CPU times: user 3min 57s, sys: 2.12 s, total: 3min 59s\n",
      "Wall time: 5min 17s\n",
      "\n",
      "4.81 Shape:  (15001, 8467) (15001,) (916, 8467) (916,)\n",
      "\n",
      "4.86 Shape:  (15001, 8467) (15001,) (916, 8467) (916,)\n",
      "\n",
      "4.91 Shape:  (15001, 8467) (15001,) (916, 8467) (916,)\n",
      "3.16 0.5698689956331878 0.6047235867956449\n",
      "3.16 [[376 200  54]\n",
      " [ 44  83  15]\n",
      " [ 30  51  63]]\n",
      "\n",
      "\n",
      "\n",
      "1.96 0.5469432314410481 0.5825593641445567\n",
      "4.96 8467\n",
      "1.96 [[353 228  49]\n",
      " [ 44  90   8]\n",
      " [ 34  52  58]]\n",
      "\n",
      "\n",
      "\n",
      "5.01 8467\n",
      "3.21 0.5665938864628821 0.6012063786388593\n",
      "3.21 [[372 200  58]\n",
      " [ 43  83  16]\n",
      " [ 31  49  64]]\n",
      "\n",
      "\n",
      "\n",
      "5.06 8467\n",
      "CPU times: user 4min 1s, sys: 2.49 s, total: 4min 4s\n",
      "Wall time: 5min 12s\n",
      "\n",
      "CPU times: user 3min 38s, sys: 1.76 s, total: 3min 40s\n",
      "Wall time: 5min 24s\n",
      "4.96 Shape:  (15001, 8467) (15001,) (916, 8467) (916,)\n",
      "CPU times: user 3min 43s, sys: 1.92 s, total: 3min 45s\n",
      "Wall time: 5min 25s\n",
      "\n",
      "5.01 Shape:  (15001, 8467) (15001,) (916, 8467) (916,)\n",
      "\n",
      "5.06 Shape:  (15001, 8467) (15001,) (916, 8467) (916,)\n",
      "3.11 0.5698689956331878 0.6047235867956449\n",
      "3.11 [[376 200  54]\n",
      " [ 44  83  15]\n",
      " [ 30  51  63]]\n",
      "\n",
      "\n",
      "\n",
      "5.11 8467\n",
      "\n",
      "5.11 Shape:  (15001, 8467) (15001,) (916, 8467) (916,)\n",
      "3.51 0.5578602620087336 0.5955535127578561\n",
      "3.51 [[366 204  60]\n",
      " [ 46  80  16]\n",
      " [ 24  55  65]]\n",
      "\n",
      "\n",
      "\n",
      "5.16 8467\n",
      "2.01 0.5567685589519651 0.5929228382982278\n",
      "2.01 [[355 221  54]\n",
      " [ 41  90  11]\n",
      " [ 29  50  65]]\n",
      "\n",
      "\n",
      "\n",
      "5.21 8467\n",
      "2.16 0.5698689956331878 0.604401570481894\n",
      "2.16 [[369 205  56]\n",
      " [ 45  87  10]\n",
      " [ 28  50  66]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5.16 Shape:  (15001, 8467) (15001,) (916, 8467) (916,)\n",
      "5.26 9796\n",
      "\n",
      "5.21 Shape:  (15001, 8467) (15001,) (916, 8467) (916,)\n",
      "CPU times: user 3min 54s, sys: 1.83 s, total: 3min 56s\n",
      "Wall time: 5min 39s\n",
      "\n",
      "5.26 Shape:  (15001, 9796) (15001,) (916, 9796) (916,)\n",
      "2.36 0.5589519650655022 0.5944440046952109\n",
      "2.36 [[369 205  56]\n",
      " [ 43  84  15]\n",
      " [ 31  54  59]]\n",
      "\n",
      "\n",
      "\n",
      "5.31 9796\n",
      "CPU times: user 3min 57s, sys: 2.96 s, total: 4min\n",
      "Wall time: 5min 12s\n",
      "CPU times: user 3min 55s, sys: 2.14 s, total: 3min 57s\n",
      "Wall time: 5min 43s\n",
      "CPU times: user 3min 57s, sys: 1.99 s, total: 3min 59s\n",
      "Wall time: 5min 44s\n",
      "CPU times: user 4min 3s, sys: 2.02 s, total: 4min 5s\n",
      "Wall time: 5min 46s\n",
      "\n",
      "5.31 Shape:  (15001, 9796) (15001,) (916, 9796) (916,)\n",
      "CPU times: user 3min 55s, sys: 1.8 s, total: 3min 56s\n",
      "Wall time: 5min 49s\n",
      "CPU times: user 3min 54s, sys: 2.4 s, total: 3min 57s\n",
      "Wall time: 5min 51s\n",
      "2.61 0.5578602620087336 0.594150784354611\n",
      "2.61 [[363 214  53]\n",
      " [ 44  83  15]\n",
      " [ 31  48  65]]\n",
      "\n",
      "\n",
      "\n",
      "5.36 9796\n",
      "3.66 0.5578602620087336 0.5961565998357328\n",
      "3.66 [[367 205  58]\n",
      " [ 47  79  16]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "2.86 0.5633187772925764 0.5991255406150069\n",
      "2.86 [[374 201  55]\n",
      " [ 46  79  17]\n",
      " [ 30  51  63]]\n",
      "\n",
      "\n",
      "\n",
      "5.41 9796\n",
      "2.51 0.5534934497816594 0.5911419602771514\n",
      "2.51 [[364 211  55]\n",
      " [ 44  80  18]\n",
      " [ 29  52  63]]\n",
      "\n",
      "\n",
      "5.46 9796\n",
      "\n",
      "5.51 9796\n",
      "\n",
      "5.41 Shape:  (15001, 9796) (15001,) (916, 9796) (916,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5.36 Shape:  (15001, 9796) (15001,) (916, 9796) (916,)\n",
      "CPU times: user 3min 57s, sys: 1.73 s, total: 3min 59s\n",
      "\n",
      "Wall time: 5min 56s\n",
      "5.46 Shape:  (15001, 9796) (15001,) (916, 9796) (916,)\n",
      "2.56 0.5600436681222707 0.5973684982660223\n",
      "2.56 [[366 212  52]\n",
      " [ 43  83  16]\n",
      " [ 28  52  64]]\n",
      "\n",
      "\n",
      "\n",
      "5.56 9796\n",
      "\n",
      "5.51 Shape:  (15001, 9796) (15001,) (916, 9796) (916,)\n",
      "2.26 0.5698689956331878 0.6044473288264998\n",
      "2.26 [[373 199  58]\n",
      " [ 41  85  16]\n",
      " [ 30  50  64]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5.56 Shape:  (15001, 9796) (15001,) (916, 9796) (916,)\n",
      "CPU times: user 4min 7s, sys: 2.1 s, total: 4min 9s\n",
      "Wall time: 6min 1s\n",
      "CPU times: user 4min 9s, sys: 1.94 s, total: 4min 11s\n",
      "Wall time: 6min 3s\n",
      "CPU times: user 4min 5s, sys: 1.82 s, total: 4min 6s\n",
      "Wall time: 6min 1s\n",
      "CPU times: user 4min 13s, sys: 2.01 s, total: 4min 15s\n",
      "Wall time: 6min 2s\n",
      "2.46 0.5665938864628821 0.6023101215632166\n",
      "2.46 [[371 202  57]\n",
      " [ 43  85  14]\n",
      " [ 27  54  63]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 4min 8s, sys: 2.53 s, total: 4min 10s\n",
      "Wall time: 5min 31s\n",
      "CPU times: user 4min 9s, sys: 1.89 s, total: 4min 10s\n",
      "Wall time: 6min 6s\n",
      "CPU times: user 4min 13s, sys: 2.04 s, total: 4min 15s\n",
      "Wall time: 6min 9s\n",
      "CPU times: user 4min 15s, sys: 1.82 s, total: 4min 16s\n",
      "Wall time: 6min 10s\n",
      "3.41 0.5600436681222707 0.5976049450457055\n",
      "3.41 [[368 203  59]\n",
      " [ 46  80  16]\n",
      " [ 24  55  65]]3.01 0.5644104803493449 0.6005280482408965\n",
      "\n",
      "3.01 [[374 205  51]\n",
      " [ 46  80  16]\n",
      " [ 30  51  63]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.26 0.5665938864628821 0.6012063786388593\n",
      "3.26 [[372 200  58]\n",
      " [ 43  83  16]\n",
      " [ 31  49  64]]\n",
      "\n",
      "\n",
      "\n",
      "2.81 0.5611353711790393 0.5979407247786217\n",
      "2.81 [[370 207  53]\n",
      " [ 45  80  17]\n",
      " [ 29  51  64]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 4min 14s, sys: 2.09 s, total: 4min 16s\n",
      "Wall time: 6min 14s\n",
      "3.06 0.5687772925764192 0.6038148809964368\n",
      "3.06 [[375 200  55]\n",
      " [ 45  81  16]\n",
      " [ 30  49  65]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 4min 13s, sys: 2.17 s, total: 4min 15s\n",
      "Wall time: 6min 4s\n",
      "3.71 0.5567685589519651 0.5951603990161926\n",
      "3.71 [[368 204  58]\n",
      " [ 48  78  16]\n",
      " [ 23  57  64]]\n",
      "\n",
      "\n",
      "\n",
      "3.31 0.5665938864628821 0.6028118438224148\n",
      "3.36 0.5611353711790393 0.5977433952206487\n",
      "3.31 [[373 199  58]\n",
      " [ 45  81  16]\n",
      " [ 26  53  65]]3.36 [[369 201  60]\n",
      " [ 46  80  16]\n",
      " [ 26  53  65]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.46 0.5578602620087336 0.5955535127578561\n",
      "3.46 [[366 204  60]\n",
      " [ 46  80  16]\n",
      " [ 24  55  65]]\n",
      "\n",
      "\n",
      "\n",
      "3.56 0.5578602620087336 0.5954505268546842\n",
      "3.56 [[368 204  58]\n",
      " [ 48  78  16]\n",
      " [ 25  54  65]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 4min 17s, sys: 1.84 s, total: 4min 19s\n",
      "Wall time: 6min 12s\n",
      "3.61 0.5578602620087336 0.5961565998357328\n",
      "3.61 [[367 205  58]\n",
      " [ 47  79  16]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 4min 15s, sys: 2.05 s, total: 4min 17s\n",
      "Wall time: 5min 58s\n",
      "3.76 0.5567685589519651 0.5951603990161926\n",
      "3.76 [[368 204  58]\n",
      " [ 48  78  16]\n",
      " [ 23  57  64]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 4min 10s, sys: 1.72 s, total: 4min 11s\n",
      "Wall time: 5min 36s\n",
      "3.81 0.5567685589519651 0.5951603990161926\n",
      "3.81 [[368 204  58]\n",
      " [ 48  78  16]\n",
      " [ 23  57  64]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 4min 8s, sys: 1.68 s, total: 4min 10s\n",
      "Wall time: 5min 24s\n",
      "3.86 0.5567685589519651 0.5951603990161926\n",
      "3.86 [[368 204  58]\n",
      " [ 48  78  16]\n",
      " [ 23  57  64]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 4min 23s, sys: 2.16 s, total: 4min 25s\n",
      "Wall time: 5min 22s\n",
      "3.91 0.5600436681222707 0.5985567623021284\n",
      "3.91 [[368 205  57]\n",
      " [ 47  79  16]\n",
      " [ 22  56  66]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 3min 52s, sys: 2.02 s, total: 3min 54s\n",
      "Wall time: 4min 24s\n",
      "4.11 0.5644104803493449 0.6017170348409955\n",
      "4.11 [[371 201  58]\n",
      " [ 46  79  17]\n",
      " [ 24  53  67]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 3min 57s, sys: 2.1 s, total: 3min 59s\n",
      "Wall time: 4min 31s\n",
      "4.21 0.5622270742358079 0.6007384105919839\n",
      "4.21 [[372 203  55]\n",
      " [ 46  78  18]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 4min 24s, sys: 2.8 s, total: 4min 27s\n",
      "Wall time: 5min 10s\n",
      "CPU times: user 4min 31s, sys: 2.07 s, total: 4min 34s\n",
      "Wall time: 5min 7s\n",
      "CPU times: user 4min 20s, sys: 2.56 s, total: 4min 23s\n",
      "Wall time: 5min 1s\n",
      "CPU times: user 3min 53s, sys: 1.74 s, total: 3min 55s\n",
      "Wall time: 4min 26s\n",
      "3.96 0.5600436681222707 0.5985567623021284\n",
      "3.96 [[368 205  57]\n",
      " [ 47  79  16]\n",
      " [ 22  56  66]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 4min 22s, sys: 2.18 s, total: 4min 24s\n",
      "Wall time: 4min 56s\n",
      "4.06 0.5589519650655022 0.5975736191739699\n",
      "4.06 [[368 205  57]\n",
      " [ 48  78  16]\n",
      " [ 22  56  66]]\n",
      "\n",
      "\n",
      "\n",
      "4.01 0.5589519650655022 0.5975736191739699\n",
      "4.01 [[368 205  57]\n",
      " [ 48  78  16]\n",
      " [ 22  56  66]]\n",
      "\n",
      "\n",
      "\n",
      "4.46 0.5622270742358079 0.6007384105919839\n",
      "4.46 [[372 203  55]\n",
      " [ 46  78  18]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "4.16 0.5644104803493449 0.6017170348409955\n",
      "4.16 [[371 201  58]\n",
      " [ 46  79  17]\n",
      " [ 24  53  67]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 3min 53s, sys: 1.79 s, total: 3min 54s\n",
      "Wall time: 4min 16s\n",
      "CPU times: user 4min 8s, sys: 2.1 s, total: 4min 11s\n",
      "Wall time: 4min 45s\n",
      "CPU times: user 3min 52s, sys: 2.28 s, total: 3min 54s\n",
      "Wall time: 4min 15s\n",
      "4.81 0.5633187772925764 0.601949759621957\n",
      "4.81 [[370 206  54]\n",
      " [ 43  81  18]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "4.41 0.5622270742358079 0.6007384105919839\n",
      "4.41 [[372 203  55]\n",
      " [ 46  78  18]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "5.01 0.5633187772925764 0.601949759621957\n",
      "5.01 [[370 206  54]\n",
      " [ 43  81  18]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 3min 55s, sys: 1.72 s, total: 3min 56s\n",
      "Wall time: 4min 14s\n",
      "CPU times: user 4min 30s, sys: 2.31 s, total: 4min 33s\n",
      "Wall time: 5min 5s\n",
      "CPU times: user 3min 54s, sys: 1.86 s, total: 3min 56s\n",
      "Wall time: 4min 14s\n",
      "CPU times: user 3min 51s, sys: 2.01 s, total: 3min 53s\n",
      "Wall time: 4min 6s\n",
      "CPU times: user 4min 37s, sys: 2.02 s, total: 4min 39s\n",
      "Wall time: 5min 11s\n",
      "CPU times: user 4min 20s, sys: 2.5 s, total: 4min 23s\n",
      "Wall time: 4min 52s\n",
      "5.21 0.5633187772925764 0.601949759621957\n",
      "5.21 [[370 206  54]\n",
      " [ 43  81  18]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "5.26 0.5644104803493449 0.6028649904389555\n",
      "5.26 [[371 207  52]\n",
      " [ 43  81  18]\n",
      " [ 24  55  65]]\n",
      "\n",
      "\n",
      "\n",
      "4.31 0.5622270742358079 0.6007384105919839\n",
      "4.31 [[372 203  55]\n",
      " [ 46  78  18]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "5.31 0.5644104803493449 0.6028649904389555\n",
      "5.31 [[371 207  52]\n",
      " [ 43  81  18]\n",
      " [ 24  55  65]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 4min 28s, sys: 2.6 s, total: 4min 31s\n",
      "Wall time: 4min 54s\n",
      "CPU times: user 4min 36s, sys: 2.08 s, total: 4min 38s\n",
      "Wall time: 5min 10s\n",
      "4.56 0.5633187772925764 0.6017288915017913\n",
      "4.56 [[371 206  53]\n",
      " [ 44  80  18]\n",
      " [ 24  55  65]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 3min 46s, sys: 1.92 s, total: 3min 48s\n",
      "Wall time: 3min 59s\n",
      "4.26 0.5622270742358079 0.6007384105919839\n",
      "4.26 [[372 203  55]\n",
      " [ 46  78  18]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 3min 51s, sys: 1.87 s, total: 3min 53s\n",
      "Wall time: 4min 6s\n",
      "5.56 0.5644104803493449 0.6028649904389555\n",
      "5.56 [[371 207  52]\n",
      " [ 43  81  18]\n",
      " [ 24  55  65]]\n",
      "\n",
      "\n",
      "\n",
      "4.61 0.5633187772925764 0.601949759621957\n",
      "4.61 [[370 206  54]\n",
      " [ 43  81  18]\n",
      " [ 23  56  65]]4.36 0.5622270742358079 0.6007384105919839\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4.36 [[372 203  55]\n",
      " [ 46  78  18]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 3min 56s, sys: 1.8 s, total: 3min 57s\n",
      "Wall time: 4min 8s\n",
      "CPU times: user 3min 55s, sys: 2.06 s, total: 3min 57s\n",
      "Wall time: 4min 8s\n",
      "5.46 0.5644104803493449 0.6028649904389555\n",
      "5.46 [[371 207  52]\n",
      " [ 43  81  18]\n",
      " [ 24  55  65]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 4min 38s, sys: 2.7 s, total: 4min 41s\n",
      "Wall time: 5min 8s\n",
      "CPU times: user 4min 31s, sys: 2.69 s, total: 4min 33s\n",
      "Wall time: 4min 59s\n",
      "5.36 0.5644104803493449 0.6028649904389555\n",
      "5.36 [[371 207  52]\n",
      " [ 43  81  18]\n",
      " [ 24  55  65]]\n",
      "\n",
      "\n",
      "\n",
      "5.51 0.5644104803493449 0.6028649904389555\n",
      "5.51 [[371 207  52]\n",
      " [ 43  81  18]\n",
      " [ 24  55  65]]\n",
      "\n",
      "\n",
      "\n",
      "4.66 0.5633187772925764 0.601949759621957\n",
      "4.66 [[370 206  54]\n",
      " [ 43  81  18]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "4.51 0.5622270742358079 0.6007384105919839\n",
      "4.51 [[372 203  55]\n",
      " [ 46  78  18]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 4min 27s, sys: 2.31 s, total: 4min 29s\n",
      "Wall time: 4min 53s\n",
      "CPU times: user 4min 33s, sys: 2.06 s, total: 4min 35s\n",
      "Wall time: 5min 4s\n",
      "CPU times: user 4min 33s, sys: 2.24 s, total: 4min 36s\n",
      "Wall time: 5min 5s\n",
      "4.86 0.5633187772925764 0.601949759621957\n",
      "4.86 [[370 206  54]\n",
      " [ 43  81  18]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "4.71 0.5633187772925764 0.601949759621957\n",
      "4.71 [[370 206  54]\n",
      " [ 43  81  18]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 4min 17s, sys: 3.88 s, total: 4min 21s\n",
      "Wall time: 4min 49s\n",
      "4.76 0.5633187772925764 0.601949759621957\n",
      "4.76 [[370 206  54]\n",
      " [ 43  81  18]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 4min 34s, sys: 1.94 s, total: 4min 36s\n",
      "Wall time: 5min\n",
      "CPU times: user 4min 27s, sys: 2.27 s, total: 4min 29s\n",
      "Wall time: 4min 56s\n",
      "CPU times: user 4min 28s, sys: 1.88 s, total: 4min 30s\n",
      "Wall time: 4min 54s\n",
      "CPU times: user 4min 19s, sys: 2.34 s, total: 4min 21s\n",
      "Wall time: 4min 46s\n",
      "5.11 0.5633187772925764 0.601949759621957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.11 [[370 206  54]\n",
      " [ 43  81  18]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "4.91 0.5633187772925764 0.601949759621957\n",
      "4.91 [[370 206  54]\n",
      " [ 43  81  18]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "4.96 0.5633187772925764 0.601949759621957\n",
      "4.96 [[370 206  54]\n",
      " [ 43  81  18]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "5.16 0.5633187772925764 0.601949759621957\n",
      "5.16 [[370 206  54]\n",
      " [ 43  81  18]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "5.06 0.5633187772925764 0.601949759621957\n",
      "5.06 [[370 206  54]\n",
      " [ 43  81  18]\n",
      " [ 23  56  65]]\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 4min 13s, sys: 2.88 s, total: 4min 16s\n",
      "Wall time: 4min 38s\n",
      "5.41 0.5644104803493449 0.6028649904389555\n",
      "5.41 [[371 207  52]\n",
      " [ 43  81  18]\n",
      " [ 24  55  65]]\n",
      "\n",
      "\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def gimme_words_over_a_certain_threshold(threshold, min_df):\n",
    "    vocabulary = []\n",
    "    for word in word_entropy_dict:\n",
    "        if word_entropy_dict[word] < threshold and sum(word_count_dict[word]) > min_df:\n",
    "            vocabulary.append(word)\n",
    "\n",
    "    print(threshold, len(vocabulary))\n",
    "    return vocabulary\n",
    "    \n",
    "    \n",
    "def something(stop_word_threshold, min_df = 3):\n",
    "    vocabulary_for_train_data = gimme_words_over_a_certain_threshold(stop_word_threshold, min_df)\n",
    "    if len(vocabulary_for_train_data) < 5:\n",
    "        return\n",
    "\n",
    "    dummy_tfidf = TfidfVectorizer(tokenizer=tweeter.tokenize,\n",
    "                                vocabulary=vocabulary_for_train_data)\n",
    "\n",
    "    X_train = dummy_tfidf.fit_transform(train_pd.Data)\n",
    "    y_train = train_pd.Label_num\n",
    "    X_test = dummy_tfidf.transform(test_pd.Data)\n",
    "    y_test = test_pd.Label_num\n",
    "#     count = 10\n",
    "#     for bleh in dummy_tfidf.vocabulary_:\n",
    "#         print(bleh, end=', ')\n",
    "#         count -= 1\n",
    "#         if count == 0:\n",
    "#             break\n",
    "    print()\n",
    "    print(stop_word_threshold, \"Shape: \", X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "#     X_train, y_train, X_test, y_test = remove_irrelevant_samples(X_train.toarray(), y_train, X_test.toarray(), y_test)\n",
    "\n",
    "    svm_classifier = SVC(kernel='linear')\n",
    "    %time svm_classifier.fit(X_train, y_train)\n",
    "    y_pred = svm_classifier.predict(X_test)\n",
    "    print(stop_word_threshold, accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='weighted'))\n",
    "    print(stop_word_threshold, confusion_matrix(y_test, y_pred))\n",
    "    print('\\n\\n')\n",
    "    \n",
    "from multiprocessing import Pool\n",
    "process_pool = Pool(processes=50)\n",
    "\n",
    "for j in range(1, 560, 5):\n",
    "    process_pool.apply_async(something, args=(j/100, 0))\n",
    "# something(8.5)\n",
    "\n",
    "process_pool.close()\n",
    "process_pool.join()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# #NAG\n",
    "# for row_number in range(len(train_pd)):\n",
    "#     row = train_pd.iloc[row_number]\n",
    "#     for word in set(tweeter.tokenize(row.Data)):\n",
    "#         if word not in word_set:\n",
    "#             word_set.add(word)\n",
    "#             word_count_dict[word] = [0, 0, 0]\n",
    "#         word_count_dict[word][row.Label_num-1] += 1\n",
    "\n",
    "from functools import reduce\n",
    "def entropy(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    return x*np.log(x)\n",
    "\n",
    "# word_entropy_dict = {}\n",
    "# a = []\n",
    "# b = []    \n",
    "\n",
    "\n",
    "word_count_dict = {}\n",
    "word_set = set()\n",
    "word_entropy_dict = {}\n",
    "\n",
    "def populate_word_count(vect, data, classy):\n",
    "    global word_count_dict, word_set, word_entropy_dict\n",
    "    vocab = vect.vocabulary_\n",
    "#     ordered_dict = {}\n",
    "    feature_count_list = vect.transform(data).toarray().sum(axis=0)\n",
    "    for char in vocab:\n",
    "        if char not in word_set:\n",
    "            word_set.add(char)\n",
    "            word_count_dict[char] = [0, 0, 0]\n",
    "        word_count_dict[char][classy-1] += feature_count_list[vocab[char]]\n",
    "#         vocab[char] = feature_count_list[vocab[char]]\n",
    "#     for i in range(feature_count_list.shape[0]):\n",
    "#         maxy = max(vocab.values())\n",
    "#         character_for_maxy = reverse_search_dictionary(vocab, maxy)\n",
    "#         vocab[character_for_maxy] = -1\n",
    "#         ordered_dict[character_for_maxy] = i\n",
    "# #         max_index = np.argmax(feature_count_list)\n",
    "# #         ordered_dict[reverse_search_dictionary(vocab, max_index)] = max_index\n",
    "# #         feature_count_list[max_index] = -1\n",
    "#     return ordered_dict\n",
    "    \n",
    "\n",
    "def populate_entropy_dict(count_vect):\n",
    "    global word_entropy_dict, word_count_dict, word_set\n",
    "    from copy import deepcopy\n",
    "    #declare CountVectorizer() \n",
    "    class_1_vectorizer = deepcopy(count_vect)\n",
    "    class_2_vectorizer = deepcopy(count_vect)\n",
    "    class_3_vectorizer = deepcopy(count_vect)\n",
    "    \n",
    "    #fit\n",
    "    class_1_vectorizer.fit(class_wise_train_data[1].Data)\n",
    "    class_2_vectorizer.fit(class_wise_train_data[2].Data)\n",
    "    class_3_vectorizer.fit(class_wise_train_data[3].Data)\n",
    "    \n",
    "    #get the features in a ordered manner\n",
    "    word_set = set()\n",
    "    word_count_dict = {}\n",
    "    word_entropy_dict = {}\n",
    "    populate_word_count(class_1_vectorizer, class_wise_train_data[1].Data, 1)\n",
    "    populate_word_count(class_2_vectorizer, class_wise_train_data[2].Data, 2)\n",
    "    populate_word_count(class_3_vectorizer, class_wise_train_data[3].Data, 3)\n",
    "    \n",
    "    for word in word_count_dict:\n",
    "        total_words = np.sum(word_count_dict[word])\n",
    "        word_entropy_dict[word] = np.abs(np.sum(list(map(lambda x: entropy(x/total_words), np.array(word_count_dict[word])))))\n",
    "\n",
    "\n",
    "def get_vocabulary_without_stopwords_for_training_data(vect, threshold):\n",
    "    #get order of features\n",
    "    populate_entropy_dict(vect)\n",
    "#     vocabulary = []\n",
    "#     for word in word_entropy_dict:\n",
    "#         if word_entropy_dict[word] < threshold and sum(word_count_dict[word]) > min_df:\n",
    "#             vocabulary.append(word)\n",
    "    listy = []\n",
    "    for word in word_entropy_dict:\n",
    "        if word_entropy_dict[word] < threshold and sum(word_count_dict[word]) > min_df:# and word_entropy_dict[word] > 0:\n",
    "            listy.append( (word, word_entropy_dict[word]))\n",
    "    return pd.DataFrame(listy, columns=['words', 'entropy']).sort_values('entropy').head(max_features)['words']\n",
    "    \n",
    "    \n",
    "def remove_irrelevant_samples(X_train, y_train, X_test, y_test):\n",
    "    thresh = 0\n",
    "    train_bool_array = (np.sum(X_train, axis=1) > thresh)\n",
    "    test_bool_array = (np.sum(X_test, axis=1) > thresh)\n",
    "#     print(train_bool_array)\n",
    "    return X_train[train_bool_array], y_train[train_bool_array], X_test[test_bool_array], y_test[test_bool_array]\n",
    "\n",
    "#parameters\n",
    "ngram_range = (4, 4)\n",
    "# token_pattern = '[^\\w+\\s+]' #punctuation\n",
    "# token_pattern = '[A-Z][A-Z]'\n",
    "#TfidfVectorizer(max_features=max_features, min_df=3),\n",
    "min_df = 0\n",
    "max_features = 20000\n",
    "tokenizer = TweetTokenizer().tokenize\n",
    "\n",
    "#lda info\n",
    "# from lda import LDA\n",
    "# from scipy.sparse import hstack\n",
    "# lda_vect = LDA(n_topics=190, n_iter=100)\n",
    "# count_vect = CountVectorizer()\n",
    "\n",
    "# count_train = count_vect.fit_transform(train_pd['Data'])\n",
    "# lda_train = lda_vect.fit_transform(count_train)\n",
    "# lda_test = lda_vect.transform(count_vect.transform(test_pd['Data']))\n",
    "\n",
    "\n",
    "def something(stop_word_threshold):\n",
    "    vocabulary_for_train_data = get_vocabulary_without_stopwords_for_training_data(\n",
    "\n",
    "                            CountVectorizer(tokenizer=tokenizer,\n",
    "                                            ), \n",
    "                            threshold=stop_word_threshold)\n",
    "\n",
    "\n",
    "    tfidf_vect = TfidfVectorizer(vocabulary=vocabulary_for_train_data)\n",
    "    X_train = tfidf_vect.fit_transform(train_pd['Data'])\n",
    "    y_train = train_pd['Label_num']\n",
    "\n",
    "    X_test = tfidf_vect.transform(test_pd['Data'])\n",
    "    y_test = test_pd['Label_num']\n",
    "\n",
    "    svm = SVC(kernel='linear')\n",
    "    svm.fit(X_train, train_pd['Label_num'])\n",
    "\n",
    "    y_pred = svm.predict(X_test)\n",
    "    print(stop_word_threshold,len(word_set), \"-->\", X_train.shape, \":\", accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='weighted'))\n",
    "    print(stop_word_threshold, confusion_matrix(y_test, y_pred))\n",
    "    print('\\n\\n')\n",
    "    \n",
    "from multiprocessing import Pool\n",
    "process_pool = Pool(processes=40)\n",
    "\n",
    "# for j in range(1000, 1200, 5):\n",
    "#     process_pool.apply_async(something, args=(j*0.001,))\n",
    "something(8.5)\n",
    "\n",
    "process_pool.close()\n",
    "process_pool.join()\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
