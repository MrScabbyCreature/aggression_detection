{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from IPython.display import display\n",
    "from nltk import TweetTokenizer\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label conversion dictionaries: text to num, num to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OAG': 3, 'CAG': 2, 'NAG': 1} \n",
      " {1: 'NAG', 2: 'CAG', 3: 'OAG'}\n"
     ]
    }
   ],
   "source": [
    "dic_aggression_level = {\n",
    "    'NAG' : 1,\n",
    "    'CAG' : 2,\n",
    "    'OAG' : 3\n",
    "}\n",
    "\n",
    "dic_reverse_aggression_level = {}\n",
    "for i in dic_aggression_level:\n",
    "    dic_reverse_aggression_level[dic_aggression_level[i]] = i\n",
    "    \n",
    "print(dic_aggression_level, '\\n', dic_reverse_aggression_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARING DATA WITH PANDAS\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OAG': 3, 'CAG': 2, 'NAG': 1} \n",
      " {1: 'NAG', 2: 'CAG', 3: 'OAG'}\n",
      "\n",
      "\n",
      "\\Train DATA\n",
      "OAG    8716\n",
      "NAG    6285\n",
      "Name: Label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Label</th>\n",
       "      <th>Label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>Focus on making cash available  then only  peo...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5103</th>\n",
       "      <td>She's so ignorant Megha Mukherji</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5090</th>\n",
       "      <td>Sonia I am holding Rel cap 430, please suggest...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9074</th>\n",
       "      <td>why dont u make ur room sound proof..simple</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6769</th>\n",
       "      <td>Showing everything and saying bold...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Then what happens in pantry coach dedicated fo...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>We should respect every religion. May be he wa...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7272</th>\n",
       "      <td>Car is good.. bt i must say.. i only heard 'aa...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10305</th>\n",
       "      <td>friends we have to understand the ground reali...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Bad...........</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Data Label  Label_num\n",
       "2930   Focus on making cash available  then only  peo...   OAG          3\n",
       "5103                    She's so ignorant Megha Mukherji   OAG          3\n",
       "5090   Sonia I am holding Rel cap 430, please suggest...   NAG          1\n",
       "9074         why dont u make ur room sound proof..simple   OAG          3\n",
       "6769               Showing everything and saying bold...   OAG          3\n",
       "140    Then what happens in pantry coach dedicated fo...   OAG          3\n",
       "1756   We should respect every religion. May be he wa...   OAG          3\n",
       "7272   Car is good.. bt i must say.. i only heard 'aa...   NAG          1\n",
       "10305  friends we have to understand the ground reali...   OAG          3\n",
       "45                                        Bad...........   OAG          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "TEST DATA\n",
      "NAG    630\n",
      "OAG    286\n",
      "Name: Label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Label</th>\n",
       "      <th>Label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>What will be done to the money we have ?</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Unchange the rapo rate could lead the stagnate...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PK Movie Bhagawan Shiv Ko Aapman Kiya..   I Ha...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>Thousands of people have died due to bandhs an...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>Worst F.M ever.what about 5  lakhs tax limit? ...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>Pak army rape modi daughter and go back. Now m...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>I am clean without cash as transactions are th...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>I have 5000 shares of Pnb @75.90 please tell m...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>these bhagwa terrorists can't digest their mea...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>Worst Decision and very worst implementation. ...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Data Label  Label_num\n",
       "156           What will be done to the money we have ?   NAG          1\n",
       "211  Unchange the rapo rate could lead the stagnate...   NAG          1\n",
       "13   PK Movie Bhagawan Shiv Ko Aapman Kiya..   I Ha...   NAG          1\n",
       "798  Thousands of people have died due to bandhs an...   OAG          3\n",
       "640  Worst F.M ever.what about 5  lakhs tax limit? ...   NAG          1\n",
       "568  Pak army rape modi daughter and go back. Now m...   NAG          1\n",
       "321  I am clean without cash as transactions are th...   NAG          1\n",
       "119  I have 5000 shares of Pnb @75.90 please tell m...   NAG          1\n",
       "820  these bhagwa terrorists can't digest their mea...   OAG          3\n",
       "721  Worst Decision and very worst implementation. ...   OAG          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAG    6285\n",
      "Name: Label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Label</th>\n",
       "      <th>Label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5090</th>\n",
       "      <td>Sonia I am holding Rel cap 430, please suggest...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7272</th>\n",
       "      <td>Car is good.. bt i must say.. i only heard 'aa...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11474</th>\n",
       "      <td>till now India is stuck wheather the muslims a...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6979</th>\n",
       "      <td>Thank God ...what about navigation system and ...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>BJP running MCD for ten years.  If people stil...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4029</th>\n",
       "      <td>What are the prospects for the Auto and Auto A...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4314</th>\n",
       "      <td>How soon do you think, the 5G will go live in ...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2428</th>\n",
       "      <td>Ask smith??? He said india will never gonna wi...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11012</th>\n",
       "      <td>Well we should keep in mind Baba's word Saurab...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2916</th>\n",
       "      <td>the first industrial revolution in india was b...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Data Label  Label_num\n",
       "5090   Sonia I am holding Rel cap 430, please suggest...   NAG          1\n",
       "7272   Car is good.. bt i must say.. i only heard 'aa...   NAG          1\n",
       "11474  till now India is stuck wheather the muslims a...   NAG          1\n",
       "6979   Thank God ...what about navigation system and ...   NAG          1\n",
       "391    BJP running MCD for ten years.  If people stil...   NAG          1\n",
       "4029   What are the prospects for the Auto and Auto A...   NAG          1\n",
       "4314   How soon do you think, the 5G will go live in ...   NAG          1\n",
       "2428   Ask smith??? He said india will never gonna wi...   NAG          1\n",
       "11012  Well we should keep in mind Baba's word Saurab...   NAG          1\n",
       "2916   the first industrial revolution in india was b...   NAG          1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAG    630\n",
      "Name: Label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Label</th>\n",
       "      <th>Label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>What will be done to the money we have ?</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Unchange the rapo rate could lead the stagnate...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PK Movie Bhagawan Shiv Ko Aapman Kiya..   I Ha...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>Worst F.M ever.what about 5  lakhs tax limit? ...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>Pak army rape modi daughter and go back. Now m...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>I am clean without cash as transactions are th...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>I have 5000 shares of Pnb @75.90 please tell m...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>Sir, I am looking for the this programme on TV...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Instead of watching this crap people shud show...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>Name change won't change fortune of state. Was...</td>\n",
       "      <td>NAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Data Label  Label_num\n",
       "156           What will be done to the money we have ?   NAG          1\n",
       "211  Unchange the rapo rate could lead the stagnate...   NAG          1\n",
       "13   PK Movie Bhagawan Shiv Ko Aapman Kiya..   I Ha...   NAG          1\n",
       "640  Worst F.M ever.what about 5  lakhs tax limit? ...   NAG          1\n",
       "568  Pak army rape modi daughter and go back. Now m...   NAG          1\n",
       "321  I am clean without cash as transactions are th...   NAG          1\n",
       "119  I have 5000 shares of Pnb @75.90 please tell m...   NAG          1\n",
       "524  Sir, I am looking for the this programme on TV...   NAG          1\n",
       "365  Instead of watching this crap people shud show...   NAG          1\n",
       "424  Name change won't change fortune of state. Was...   NAG          1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: Label, dtype: int64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Label</th>\n",
       "      <th>Label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Data, Label, Label_num]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: Label, dtype: int64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Label</th>\n",
       "      <th>Label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Data, Label, Label_num]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OAG    8716\n",
      "Name: Label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Label</th>\n",
       "      <th>Label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>Focus on making cash available  then only  peo...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5103</th>\n",
       "      <td>She's so ignorant Megha Mukherji</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9074</th>\n",
       "      <td>why dont u make ur room sound proof..simple</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6769</th>\n",
       "      <td>Showing everything and saying bold...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Then what happens in pantry coach dedicated fo...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>We should respect every religion. May be he wa...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10305</th>\n",
       "      <td>friends we have to understand the ground reali...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Bad...........</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3168</th>\n",
       "      <td>It's like a devil think .. what tha hell is th...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5374</th>\n",
       "      <td>Third rate dog</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Data Label  Label_num\n",
       "2930   Focus on making cash available  then only  peo...   OAG          3\n",
       "5103                    She's so ignorant Megha Mukherji   OAG          3\n",
       "9074         why dont u make ur room sound proof..simple   OAG          3\n",
       "6769               Showing everything and saying bold...   OAG          3\n",
       "140    Then what happens in pantry coach dedicated fo...   OAG          3\n",
       "1756   We should respect every religion. May be he wa...   OAG          3\n",
       "10305  friends we have to understand the ground reali...   OAG          3\n",
       "45                                        Bad...........   OAG          3\n",
       "3168   It's like a devil think .. what tha hell is th...   OAG          3\n",
       "5374                                      Third rate dog   OAG          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OAG    286\n",
      "Name: Label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Label</th>\n",
       "      <th>Label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>Thousands of people have died due to bandhs an...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>these bhagwa terrorists can't digest their mea...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>Worst Decision and very worst implementation. ...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>Wow miracle  of modi  Manmohan  started  speaking</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>Yes yes ..traffic population pollution unlivab...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>Pakistanis are not human being. They are just ...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>F. PM Manmohan Singh had 10 years now he has n...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>Hello karki , How are you?A troll paid by BJP....</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>The most useless PM India ever seen, he could ...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>When asked to speak in Parliament ran away. Sp...</td>\n",
       "      <td>OAG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Data Label  Label_num\n",
       "798  Thousands of people have died due to bandhs an...   OAG          3\n",
       "820  these bhagwa terrorists can't digest their mea...   OAG          3\n",
       "721  Worst Decision and very worst implementation. ...   OAG          3\n",
       "797  Wow miracle  of modi  Manmohan  started  speaking   OAG          3\n",
       "253  Yes yes ..traffic population pollution unlivab...   OAG          3\n",
       "819  Pakistanis are not human being. They are just ...   OAG          3\n",
       "707  F. PM Manmohan Singh had 10 years now he has n...   OAG          3\n",
       "684  Hello karki , How are you?A troll paid by BJP....   OAG          3\n",
       "755  The most useless PM India ever seen, he could ...   OAG          3\n",
       "906  When asked to speak in Parliament ran away. Sp...   OAG          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#labels\n",
    "dic_aggression_level = {\n",
    "    'NAG' : 1,\n",
    "    'CAG' : 2,\n",
    "    'OAG' : 3\n",
    "}\n",
    "\n",
    "dic_reverse_aggression_level = {}\n",
    "for i in dic_aggression_level:\n",
    "    dic_reverse_aggression_level[dic_aggression_level[i]] = i\n",
    "    \n",
    "print(dic_aggression_level, '\\n', dic_reverse_aggression_level)\n",
    "\n",
    "#train data\n",
    "train_pd = shuffle(pd.concat((pd.read_csv(\"train.csv\")[['Data', 'Label']], pd.read_csv(\"valid.csv\")[['Data', 'Label']])), random_state=20)\n",
    "# train_pd['Label'].replace('CAG', 'OAG', inplace=True)\n",
    "train_pd['Label_num'] = train_pd.Label.map(dic_aggression_level)\n",
    "\n",
    "#test data\n",
    "# test_fb_pd = shuffle(pd.read_csv(\"test_fb.csv\")[['Data', 'Label']], random_state=20)\n",
    "# test_fb_pd['Label_num'] = test_fb_pd.Label.map(dic_aggression_level)\n",
    "# test_tw_pd = shuffle(pd.read_csv(\"test_tw.csv\")[['Data', 'Label']], random_state=20)\n",
    "# test_tw_pd['Label_num'] = test_tw_pd.Label.map(dic_aggression_level)\n",
    "\n",
    "#test data\n",
    "test_pd = pd.read_csv(\"test_fb.csv\")\n",
    "test_pd.drop('ID',1,inplace=True)\n",
    "test_pd = shuffle(test_pd, random_state = 20)\n",
    "\n",
    "# merge binary classification (CAG -> OAG)\n",
    "# test_pd['Label'].replace('CAG', 'OAG', inplace=True)\n",
    "\n",
    "test_pd['Label_num'] = test_pd.Label.map(dic_aggression_level)\n",
    "\n",
    "print(\"\\n\\n\\Train DATA\")\n",
    "print(train_pd.Label.value_counts())\n",
    "display(train_pd.head(10))\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\nTEST DATA\")\n",
    "print(test_pd.Label.value_counts())\n",
    "display(test_pd.head(10))\n",
    "\n",
    "#individual classes\n",
    "class_wise_train_data = {}\n",
    "\n",
    "class_wise_test_data = {}\n",
    "\n",
    "for i in range(1,4):\n",
    "    class_wise_train_data[i] = train_pd[(train_pd['Label_num']==i)]\n",
    "    class_wise_test_data[i] = test_pd[(test_pd['Label_num']==i)]\n",
    "    print(class_wise_train_data[i].Label.value_counts())\n",
    "    display(class_wise_train_data[i].head(10))\n",
    "    print(class_wise_test_data[i].Label.value_counts())\n",
    "    display(class_wise_test_data[i].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brown Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_clustering_dict = pickle.load(open(\"brown_clustering_dictionary.pickle\", 'rb'))\n",
    "brown_clustering_keys_set = set(brown_clustering_dict.keys())\n",
    "from nltk import TweetTokenizer\n",
    "tweeter = TweetTokenizer()\n",
    "\n",
    "def get_brown_cluster(word):\n",
    "    if word in brown_clustering_keys_set:\n",
    "        return brown_clustering_dict[word]\n",
    "    return ''\n",
    "\n",
    "train_pd['Data'] = pd.Series(map(\" \".join, list(map(lambda x: tuple(map(get_brown_cluster, x)), tuple(map(tweeter.tokenize, train_pd['Data']))))))\n",
    "test_pd['Data'] = pd.Series(map(\" \".join, list(map(lambda x: tuple(map(get_brown_cluster, x)), tuple(map(tweeter.tokenize, test_pd['Data']))))))\n",
    "\n",
    "#over/under sampling\n",
    "train_pd = train_pd[train_pd['Label'] == 'OAG'][:3000].append(\n",
    "            train_pd[train_pd['Label'] == 'CAG'][:3000].append(\n",
    "            train_pd[train_pd['Label'] == 'NAG'][:3000]))\n",
    "\n",
    "print(train_pd['Data'], test_pd['Data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tokenising\n",
      "Finished removing stop words\n",
      "Finished stemming\n",
      "Finished POS tagging\n",
      "Finished recombining\n",
      "['NNS VBP NN NN NN NN . CC JJ JJ NN VBD JJ NN NN . NN NN', 'NNS VBP NN NN', 'NN PRP VBP JJ NN CD , NNS VBP', 'NN JJ VBP JJ NN NN NN NNP NN', 'NN NNS VBP NN :', 'RB VB RB NN JJ NN NN NN NNP NN NN NN , NN NNS VBP JJ NN . IN JJ NNS VBP PRP VB NN NN : NN NN NN VBP VBN NN NN JJ NN :', 'PRP VBP DT NN . MD VB JJ CC VB DT NN .', \"NN JJ NN NN MD VB NNP NN '' JJ NNP NN NNP NN NNP VBZ NNP POS NN NN NN\", 'NN NN NN NN , JJ JJ NN CD NN NN , NN RB JJ NN .', 'JJ :'] ['WP VBN NN .', 'JJ NN NN MD VB JJ NN NN NN NN NN .', 'NNP NN NN NN NNP NN VBD NNP PRP VBP JJ NN : PRP VBP JJ NNP NN NNP NN NNP NN NNP NNP NNP VBD NN : NN NN NN JJ NN NN RBR JJ JJ NN NN NN NN NNP CC VB', 'CD NN NN JJ NN NN . WP JJ NN .', 'RB NNP . NNP RB CD JJ NN NN . , VBD NN .', 'NN NN NN NN NN VBP RB . RB JJ VBP CD JJ NN NN', 'PRP VBP IN NN NN IN NN PRP VBP', 'PRP CD NN JJ NNP . CD NNS VBP VB CD VB', 'JJ JJ NN JJS NN IN JJ NN NN NN RB PRP VBP JJ NN MD VB NN NN NN JJ NN .', 'JJS NN JJ NN . CC IN IN NN . PRP VBD NN']\n",
      "15001\n",
      "CPU times: user 3.65 s, sys: 13.2 s, total: 16.8 s\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "#tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweeter = TweetTokenizer()\n",
    "process_pool = Pool(processes=55)\n",
    "pos_train = list(process_pool.map(tweeter.tokenize, train_pd['Data']))\n",
    "pos_test  = list(process_pool.map(tweeter.tokenize, test_pd['Data']))\n",
    "process_pool.close()\n",
    "print(\"Finished tokenising\")\n",
    "\n",
    "#remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def f(x):\n",
    "    return tuple(filter(lambda y: y not in stop_words, x))\n",
    "process_pool = Pool(processes=55)\n",
    "pos_train = list(process_pool.map(f, pos_train))\n",
    "pos_test = list(process_pool.map(f , pos_test))\n",
    "process_pool.close()\n",
    "print(\"Finished removing stop words\")\n",
    "\n",
    "#stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def f(x):\n",
    "    return tuple(map(ps.stem, x))\n",
    "process_pool = Pool(processes=55)\n",
    "pos_train = list(process_pool.map(f, pos_train))\n",
    "pos_test = list(process_pool.map(f , pos_test))\n",
    "process_pool.close()\n",
    "print(\"Finished stemming\")\n",
    "\n",
    "#POS tagging\n",
    "from nltk import pos_tag\n",
    "process_pool = Pool(processes=55)\n",
    "pos_train = list(process_pool.map(pos_tag, pos_train))\n",
    "pos_test = list(process_pool.map(pos_tag, pos_test))\n",
    "process_pool.close()\n",
    "print(\"Finished POS tagging\")\n",
    "\n",
    "#recombination\n",
    "# def f(x): #pos concatenation\n",
    "#     return \" \".join(list(map(\"\".join, x)))\n",
    "\n",
    "def f(x): #pos only sentences\n",
    "    return \" \".join(list(map(lambda y: y[1], x)))\n",
    "\n",
    "process_pool = Pool(processes=55)\n",
    "pos_train = list(process_pool.map(f, pos_train))\n",
    "pos_test = list(process_pool.map(f, pos_test))\n",
    "process_pool.close()\n",
    "print(\"Finished recombining\")\n",
    "\n",
    "\n",
    "print(pos_train[:10], pos_test[:10])\n",
    "print(len(pos_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaring different types of features (in terms if TFIDvectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer(input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, lowercase=True, \n",
    "#                 preprocessor=None, tokenizer=None, analyzer=’word’, stop_words=None, \n",
    "#                 token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), max_df=1.0, min_df=1, \n",
    "#                 max_features=None, vocabulary=None, binary=False, dtype=<class ‘numpy.int64’>, \n",
    "#                 norm=’l2’, use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "max_features = 20000\n",
    "\n",
    "feature_dict = {\n",
    "    'unigram' : TfidfVectorizer(max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    'bigram'  : TfidfVectorizer(ngram_range=(2,2), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    'trigram'  : TfidfVectorizer(ngram_range=(3,3), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    'quadgram'  : TfidfVectorizer(ngram_range=(4,4), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    'fivegram'  : TfidfVectorizer(ngram_range=(5,5), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '1-2_gram'  : TfidfVectorizer(ngram_range=(1, 2), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '1-3_gram'  : TfidfVectorizer(ngram_range=(1, 3), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '1-4_gram'  : TfidfVectorizer(ngram_range=(1, 4), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '2-3_gram'  : TfidfVectorizer(ngram_range=(2, 3), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '2-4_gram'  : TfidfVectorizer(ngram_range=(2, 4), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '3-4_gram'  : TfidfVectorizer(ngram_range=(3, 4), max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "#     'unigram_without_stopwords' : TfidfVectorizer(stop_words='english', \n",
    "#                                                   max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "#     'bigram_without_stopwords'  : TfidfVectorizer(ngram_range=(2,2), stop_words='english',\n",
    "#                                                   max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "#     'trigram_without_stopwords'  : TfidfVectorizer(ngram_range=(3,3), stop_words='english',\n",
    "#                                                   max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "#     'quadgram_without_stopwords'  : TfidfVectorizer(ngram_range=(4,4), stop_words='english',\n",
    "#                                                   max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "#     'fivegram_without_stopwords'  : TfidfVectorizer(ngram_range=(5,5), stop_words='english',\n",
    "#                                                   max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "#     '1-2_gram_without_stopwords'  : TfidfVectorizer(ngram_range=(1, 2), stop_words='english',\n",
    "#                                                   max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "#     '1-3_gram_without_stopwords'  : TfidfVectorizer(ngram_range=(1, 3), stop_words='english',\n",
    "#                                                   max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "#     '1-4_gram_without_stopwords'  : TfidfVectorizer(ngram_range=(1, 4), stop_words='english',\n",
    "#                                                   max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "#     '2-3_gram_without_stopwords'  : TfidfVectorizer(ngram_range=(2, 3), stop_words='english',\n",
    "#                                                   max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "#     '2-4_gram_without_stopwords'  : TfidfVectorizer(ngram_range=(2, 4), stop_words='english',\n",
    "#                                                   max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "#     '3-4_gram_without_stopwords'  : TfidfVectorizer(ngram_range=(3, 4), stop_words='english',\n",
    "#                                                   max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "    '1-1_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,1), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'1-2_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,2), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'1-3_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,3), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'1-4_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,4), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'1-5_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,5), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'1-6_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,6), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'1-7_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,7), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'1-8_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(1,8), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'2-2_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(2,2), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'2-3_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(2,3), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'2-4_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(2,4), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'2-5_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(2,5), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'2-6_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(2,6), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'2-7_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(2,7), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'2-8_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(2,8), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'3-3_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(3,3), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'3-4_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(3,4), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'3-5_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(3,5), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'3-6_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(3,6), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'3-7_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(3,7), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "\t'3-8_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(3,8), \n",
    "                                    max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "# \t'4-4_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(4,4), \n",
    "#                                     max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "# \t'4-5_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(4,5), \n",
    "#                                     max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "# \t'4-6_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(4,6), \n",
    "#                                     max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "# \t'4-7_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(4,7), \n",
    "#                                     max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "# \t'4-8_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(4,8), \n",
    "#                                     max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "# \t'5-5_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(5,5), \n",
    "#                                     max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "# \t'5-6_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(5,6), \n",
    "#                                     max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "# \t'5-7_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(5,7), \n",
    "#                                     max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "# \t'5-8_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(5,8), \n",
    "#                                     max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "# \t'6-6_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(6,6), \n",
    "#                                     max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "# \t'6-7_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(6,7), \n",
    "#                                     max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "# \t'6-8_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(6,8), \n",
    "#                                     max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "# \t'7-7_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(7,7), \n",
    "#                                     max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "# \t'7-8_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(7,8), \n",
    "#                                     max_features=max_features, min_df=3, tokenizer=TweetTokenizer().tokenize),\n",
    "# \t'8-8_char_gram'  : TfidfVectorizer(analyzer='char', ngram_range=(8,8), \n",
    "#                                     max_features=max_features, min_df=3)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra features (emoji, punctuation, capitals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_feature_dict = {\n",
    "    'char_1_gram_non_alphabet' : TfidfVectorizer(ngram_range=(1, 1), max_features=max_features, \n",
    "                                                 token_pattern='[^\\w+\\s+]'),\n",
    "    'char_2_gram_non_alphabet' : TfidfVectorizer(ngram_range=(2, 2), max_features=max_features, \n",
    "                                                 token_pattern='[^\\w+\\s+]'),\n",
    "    'char_3_gram_non_alphabet' : TfidfVectorizer(ngram_range=(3, 3), max_features=max_features, \n",
    "                                                 token_pattern='[^\\w+\\s+]'),\n",
    "    'char_4_gram_non_alphabet' : TfidfVectorizer(ngram_range=(4, 4), max_features=max_features, \n",
    "                                                 token_pattern='[^\\w+\\s+]'),\n",
    "    'char_5_gram_non_alphabet' : TfidfVectorizer(ngram_range=(5, 5), max_features=max_features, \n",
    "                                                 token_pattern='[^\\w+\\s+]'),\n",
    "    'char_6_gram_non_alphabet' : TfidfVectorizer(ngram_range=(6, 6), max_features=max_features, \n",
    "                                                 token_pattern='[^\\w+\\s+]'),\n",
    "    'char_7_gram_non_alphabet' : TfidfVectorizer(ngram_range=(7, 7), max_features=max_features, \n",
    "                                                 token_pattern='[^\\w+\\s+]'),\n",
    "    'char_8_gram_non_alphabet' : TfidfVectorizer(ngram_range=(8, 8), max_features=max_features, \n",
    "                                                 token_pattern='[^\\w+\\s+]'),\n",
    "    'capitals_char_1_gram' : TfidfVectorizer(max_features=max_features, lowercase=False,\n",
    "                        token_pattern='[A-Z]'),\n",
    "    'capitals_char_2_gram' : TfidfVectorizer(max_features=max_features, lowercase=False,\n",
    "                        token_pattern='[A-Z][A-Z]'),\n",
    "    'capitals_char_3_gram' : TfidfVectorizer(max_features=max_features, lowercase=False,\n",
    "                        token_pattern='[A-Z][A-Z][A-Z]'),\n",
    "    'capitals_char_4_gram' : TfidfVectorizer(max_features=max_features, lowercase=False,\n",
    "                        token_pattern='[A-Z][A-Z][A-Z][A-Z]'),\n",
    "    'capitals_char_5_gram' : TfidfVectorizer(max_features=max_features, lowercase=False,\n",
    "                        token_pattern='[A-Z][A-Z][A-Z][A-Z][A-Z]'),\n",
    "    'capitals_char_6_gram' : TfidfVectorizer(max_features=max_features, lowercase=False,\n",
    "                        token_pattern='[A-Z][A-Z][A-Z][A-Z][A-Z][A-Z]'),\n",
    "    'capitals_char_7_gram' : TfidfVectorizer(max_features=max_features, lowercase=False,\n",
    "                        token_pattern='[A-Z][A-Z][A-Z][A-Z][A-Z][A-Z][A-Z]'),\n",
    "    'capitals_char_8_gram' : TfidfVectorizer(max_features=max_features, lowercase=False,\n",
    "                        token_pattern='[A-Z][A-Z][A-Z][A-Z][A-Z][A-Z][A-Z][A-Z]'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_feature_dict = {\n",
    "    'unigram_pos' : TfidfVectorizer(max_features=max_features, min_df=3),\n",
    "    'bigram_pos'  : TfidfVectorizer(ngram_range=(2,2), max_features=max_features, min_df=3),\n",
    "    'trigram_pos'  : TfidfVectorizer(ngram_range=(3,3), max_features=max_features, min_df=3),\n",
    "    'quadgram_pos'  : TfidfVectorizer(ngram_range=(4,4), max_features=max_features, min_df=3),\n",
    "    'fivegram_pos'  : TfidfVectorizer(ngram_range=(5,5), max_features=max_features, min_df=3),\n",
    "    '1-2_gram_pos'  : TfidfVectorizer(ngram_range=(1, 2), max_features=max_features, min_df=3),\n",
    "    '1-3_gram_pos'  : TfidfVectorizer(ngram_range=(1, 3), max_features=max_features, min_df=3),\n",
    "    '1-4_gram_pos'  : TfidfVectorizer(ngram_range=(1, 4), max_features=max_features, min_df=3),\n",
    "    '2-3_gram_pos'  : TfidfVectorizer(ngram_range=(2, 3), max_features=max_features, min_df=3),\n",
    "    '2-4_gram_pos'  : TfidfVectorizer(ngram_range=(2, 4), max_features=max_features, min_df=3),\n",
    "    '3-4_gram_pos'  : TfidfVectorizer(ngram_range=(3, 4), max_features=max_features, min_df=3),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle Everything!!!!!!!!!!!! MWUAHAHAHAHAHAHA!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OAG    8716\n",
       "NAG    6285\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pd['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 58 files from features/\n",
      "Removed 16 files from extra_features/\n",
      "Removed 0 files from pos_features/\n",
      "Written \t(1 out of 32) \t\t 1-1_char_gram\n",
      "Written \t(2 out of 32) \t\t fivegram\n",
      "Written \t(3 out of 32) \t\t quadgram\n",
      "Written \t(4 out of 32) \t\t 3-3_char_gram\n",
      "Written \t(5 out of 32) \t\t 2-4_char_gram\n",
      "Written \t(6 out of 32) \t\t 3-7_char_gram\n",
      "Written \t(7 out of 32) \t\t 3-4_gram\n",
      "Written \t(8 out of 32) \t\t 3-5_char_gram\n",
      "Written \t(9 out of 32) \t\t 3-4_char_gram\n",
      "Written \t(10 out of 32) \t\t 1-2_gram\n",
      "Written \t(11 out of 32) \t\t 1-2_char_gram\n",
      "Written \t(12 out of 32) \t\t 1-7_char_gram\n",
      "Written \t(13 out of 32) \t\t 2-2_char_gram\n",
      "Written \t(14 out of 32) \t\t 1-3_char_gram\n",
      "Written \t(15 out of 32) \t\t 2-8_char_gram\n",
      "Written \t(16 out of 32) \t\t 1-8_char_gram\n",
      "Written \t(17 out of 32) \t\t 3-8_char_gram\n",
      "Written \t(18 out of 32) \t\t 2-5_char_gram\n",
      "Written \t(19 out of 32) \t\t 1-4_gram\n",
      "Written \t(20 out of 32) \t\t unigram\n",
      "Written \t(21 out of 32) \t\t 2-4_gram\n",
      "Written \t(22 out of 32) \t\t 1-4_char_gram\n",
      "Written \t(23 out of 32) \t\t 2-3_gram\n",
      "Written \t(24 out of 32) \t\t 1-6_char_gram\n",
      "Written \t(25 out of 32) \t\t 1-5_char_gram\n",
      "Written \t(26 out of 32) \t\t 2-7_char_gram\n",
      "Written \t(27 out of 32) \t\t bigram\n",
      "Written \t(28 out of 32) \t\t 3-6_char_gram\n",
      "Written \t(29 out of 32) \t\t trigram\n",
      "Written \t(30 out of 32) \t\t 1-3_gram\n",
      "Written \t(31 out of 32) \t\t 2-3_char_gram\n",
      "Written \t(32 out of 32) \t\t 2-6_char_gram\n",
      "CPU times: user 3min 7s, sys: 2.95 s, total: 3min 10s\n",
      "Wall time: 3min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "count = 0\n",
    "total = len(feature_dict)\n",
    "\n",
    "# # dump textual data\n",
    "pickle.dump({\"Train\": train_pd, \"Test\": test_pd}, open(\"original_data_in_text.pickle\", 'wb'))\n",
    "\n",
    "# # pre-empty directories\n",
    "for folder in [\"features/\", \"extra_features/\", \"pos_features/\"]:\n",
    "    remove_count = 0\n",
    "    for file in os.listdir(folder):\n",
    "        os.remove(os.path.join(folder, file))\n",
    "        remove_count += 1\n",
    "    print(\"Removed {} files from {}\".format(remove_count, folder))\n",
    "\n",
    "# # dump word and character n-gram tfid's\n",
    "for vect in feature_dict:\n",
    "    tfidf_vectorizer = feature_dict[vect]\n",
    "    pickle.dump({'X_train': tfidf_vectorizer.fit_transform(train_pd.Data),\n",
    "                 'y_train': train_pd.Label_num, \n",
    "                 'X_test': tfidf_vectorizer.transform(test_pd.Data), \n",
    "                 'y_test': test_pd.Label_num},\n",
    "                open(\"features/\" + vect + \".pickle\", 'wb'))\n",
    "    count += 1\n",
    "    print(\"Written \\t({} out of {}) \\t\\t {}\".format(count, total, vect))\n",
    "    \n",
    "# dump punctuation and capital n-gram tfid's\n",
    "# print(\"\\n\\n\")\n",
    "# count = 0\n",
    "# total = len(extra_feature_dict)\n",
    "# for vect in extra_feature_dict:\n",
    "#     tfidf_vectorizer = extra_feature_dict[vect]\n",
    "#     pickle.dump({'X_train': tfidf_vectorizer.fit_transform(train_pd.Data),\n",
    "#                  'y_train': train_pd.Label_num, \n",
    "#                  'X_test': tfidf_vectorizer.transform(test_pd.Data), \n",
    "#                  'y_test': test_pd.Label_num},\n",
    "#                 open(\"extra_features/\" + vect + \".pickle\", 'wb'))\n",
    "#     count += 1\n",
    "#     print(\"Written \\t({} out of {}) \\t\\t {}\".format(count, total, vect))\n",
    "    \n",
    "# # dump pos n-gram tfid's\n",
    "# print(\"\\n\\n\")\n",
    "# count = 0\n",
    "# total = len(pos_feature_dict)\n",
    "# for vect in pos_feature_dict:\n",
    "#     tfidf_vectorizer = pos_feature_dict[vect]\n",
    "#     pickle.dump({'X_train': tfidf_vectorizer.fit_transform(pos_train),\n",
    "#                  'y_train': train_pd.Label_num, \n",
    "#                  'X_test': tfidf_vectorizer.transform(pos_test), \n",
    "#                  'y_test': test_pd.Label_num},\n",
    "#                 open(\"pos_features/\" + vect + \".pickle\", 'wb'))\n",
    "#     count += 1\n",
    "#     print(\"Written \\t({} out of {}) \\t\\t {}\".format(count, total, vect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count_dict = {\n",
    "    'NAG' : 0,\n",
    "    'CAG' : 0,\n",
    "    'OAG' : 0\n",
    "}\n",
    "\n",
    "count = 10\n",
    "\n",
    "for _, row in test_pd.iterrows():\n",
    "    if row['Data'].isupper():\n",
    "        print(row['Label'], \"\\n\", row['Data'], \"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
